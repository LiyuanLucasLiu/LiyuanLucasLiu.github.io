[{"authors":["liyuan-liu"],"categories":null,"content":"Welcome to Lucas Liyuan Liu (刘力源)\u0026lsquo;s webpage! I am a Ph.D. student at the University of Illinois at Urbana-Champaign (UIUC), advised by Prof. Jiawei Han. I received B.Eng. from the University of Science and Technology of China (USTC), majoring in Computer Science. My undergraduate advisor is Prof. Linli Xu.\nIf you are going to visit UIUC, please let me buy you a bubble tea . PS: House Milk Teak and Black Sugar Milk Teak (w. no additional sugar) are pretty good.\n","date":1580515200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580515200,"objectID":"5329f68875b61a2fd3f27d141036c39a","permalink":"https://liyuanlucasliu.github.io/authors/liyuan-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liyuan-liu/","section":"authors","summary":"Welcome to Lucas Liyuan Liu (刘力源)\u0026lsquo;s webpage! I am a Ph.D. student at the University of Illinois at Urbana-Champaign (UIUC), advised by Prof. Jiawei Han. I received B.Eng. from the University of Science and Technology of China (USTC), majoring in Computer Science. My undergraduate advisor is Prof. Linli Xu.\nIf you are going to visit UIUC, please let me buy you a bubble tea . PS: House Milk Teak and Black Sugar Milk Teak (w.","tags":null,"title":"Liyuan Liu","type":"authors"},{"authors":null,"categories":null,"content":"My Macbook Pro 2015 system breaks from an unsuccessful program install, and time machine saves me, again.\nData is priceless and duplication is the only way to protect it. Time Machine is a useful program, however, its speed could be deadly slow if you are using a remote storage (instead of directly connected via usb or other cable).\nAfter many trial and error attempts, my current solution is:\n use NextCloud to sync codes, images, and other non-application non-system files. If you are also a synology user, this platform is much better than Drive 2.0; use Time Machine to backup only system files and applications, with the speed-up setting as below.  Even only a small portion of my files (~100 Gb) is handled by Time Machine, they could be a very slow 100 Gb without speeding up. The reason is the system sets an upper bound on the performance of Time Machine by default. Running the following code would save you tons of time:\nsudo sysctl debug.lowpri_throttle_enabled=0  You can change it back after the first \u0026amp; major backups with:\nsudo sysctl debug.lowpri_throttle_enabled=1  ","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582934400,"objectID":"5117b479586eb67cc94e2c5e513da0ca","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-tm/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/blog/2020-02-tm/","section":"blog","summary":"Time Machine, why are you sooo slooow and how to speed you up.","tags":["DIY","NAS","Time Machine"],"title":"How to Save You (Make You Fast), Time Machine","type":"blog"},{"authors":null,"categories":null,"content":"Since I need a router to turn the printer (gift from Jingbo) into a internet printer, I did some research on cheap-but-powerful routers.\nK3C costs about 30$ but performs like a 200$ router.\nUpgrading to the un-official firearm is highly recommended, which unlocks tons of features, including Ads-blocking, USB-printer, etc.\n A nice tutorial is available in Chinese. However, no available tutorial is for K3C-international (i.e., can be brought from U.S.). After some tiny-little explorations, I successfully finished the upgrade. Here are some notes based on my experience:\n upgrading to unofficial versions is risky; all operations are at your own risk; the version number of international-K3C firearm is in a different serial (from the tutorial), so ignore all version information in the tutorial; update the official firearm to the latest version, and then do the upgrade (I did this and it worked for me); after upgrade to unofficial 1.1, please go ahead and upgrade to unofficial 1.6 (1.7 is not very stable and not recommended); you need a windows pc for the upgrade\u0026hellip; (based on the tutorial); some links in the tutorial page is outdated, please find the files as below:  Necessary files:\n Telnet Tool V1.1 Firearm V1.6 Firearm  ","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582675200,"objectID":"b0546a321a360b7c81fbb9cb27a328be","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-zotero/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/blog/2020-02-zotero/","section":"blog","summary":"How to Importing Documents from Mendeley (Un-official Tricks).","tags":["DIY","NAS","Zotero"],"title":"Zotero, Importing from Mendeley","type":"blog"},{"authors":null,"categories":null,"content":"Since I need a router to turn the printer (gift from Jingbo) into a internet printer, I did some research on cheap-but-powerful routers.\nK3C costs about 30$ but performs like a 200$ router.\nUpgrading to the un-official firearm is highly recommended, which unlocks tons of features, including Ads-blocking, USB-printer, etc.\n A nice tutorial is available in Chinese. However, no available tutorial is for K3C-international (i.e., can be brought from U.S.). After some tiny-little explorations, I successfully finished the upgrade. Here are some notes based on my experience:\n upgrading to unofficial versions is risky; all operations are at your own risk; the version number of international-K3C firearm is in a different serial (from the tutorial), so ignore all version information in the tutorial; update the official firearm to the latest version, and then do the upgrade (I did this and it worked for me); after upgrade to unofficial 1.1, please go ahead and upgrade to unofficial 1.6 (1.7 is not very stable and not recommended); you need a windows pc for the upgrade\u0026hellip; (based on the tutorial); some links in the tutorial page is outdated, please find the files as below:  Necessary files:\n Telnet Tool V1.1 Firearm V1.6 Firearm  ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"05c1be7b3ee93d34e7ccb52d157fc230","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-k3c/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/blog/2020-02-k3c/","section":"blog","summary":"How to upgrade the firearm to the un-official version for K3C-international.","tags":["DIY","Router"],"title":"Hands Down, Best Router within 100$: K3C (which costs 30 USD)","type":"blog"},{"authors":["Jingbo Shang*","Xinyang Zhang*","Liyuan Liu","Sha Li","Jiawei Han"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"a79fac521f6a13a97fe0b63da9aa8c4b","permalink":"https://liyuanlucasliu.github.io/publication/shang-nettaxo-2020/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/shang-nettaxo-2020/","section":"publication","summary":"The automated construction of topic taxonomies can benefit numerous applications, including web search, recommendation, and knowledge discovery. One of the major advantages of automatic taxonomy construction is the ability to capture corpus-specific information and adapt to different scenarios. To better reflect the characteristics of a corpus, we take the meta-data of documents into consideration and view the corpus as a text-rich network. In this paper, we propose NetTaxo, a novel automatic topic taxonomy construction framework, which goes beyond the existing paradigm and allows text data to collaborate with network structure. Specifically, we learn term embeddings from both text and network as contexts. Network motifs are adopted to capture appropriate network contexts. We conduct an instance-level selection for motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node. Extensive experiments on two real-world datasets demonstrate the superiority of our method over the state-of-the-art, and further verify the effectiveness and importance of instance-level motif selection.","tags":["Taxonomies","Network"],"title":"NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network","type":"publication"},{"authors":["Liyuan Liu","Haoming Jiang","Pengcheng He","Weizhu Chen","Xiaodong Liu","Jianfeng Gao","Jiawei Han"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"3c6978df7609dc44500a46bcad7d634f","permalink":"https://liyuanlucasliu.github.io/publication/liu-radam-2020/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/liu-radam-2020/","section":"publication","summary":"The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.","tags":["Warmup","Optimization","Selected"],"title":"On the Variance of the Adaptive Learning Rate and Beyond","type":"publication"},{"authors":["Zihan Wang*","Jingbo Shang*","Liyuan Liu","Lihao Lu","Jiacheng Liu","Jiawei Han"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bbe5da5bfc94a68b2fcc0e1ec4f0ff6f","permalink":"https://liyuanlucasliu.github.io/publication/wang-crossweigh-2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/wang-crossweigh-2019/","section":"publication","summary":"Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh.","tags":["NER","Label Mistake","CoNLL03","Selected","Evaluation"],"title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations","type":"publication"},{"authors":["Qinyuan Ye*","Liyuan Liu","Maosen Zhang","Xiang Ren"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d7535dc1789bf45d9c296bcb48fc53ad","permalink":"https://liyuanlucasliu.github.io/publication/ye-looking-2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/ye-looking-2019/","section":"publication","summary":"In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23% relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.","tags":["Shifted Label Distribution","Relation Extraction","Distant Supervision","Selected"],"title":"Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Ouyu Lan","Xiao Huang","Bill Yuchen Lin","He Jiang","Liyuan Liu","Xiang Ren"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"4fbbc0669f309539ba731a88b00fbe83","permalink":"https://liyuanlucasliu.github.io/publication/lan-learning-2019/","publishdate":"2020-02-28T21:51:58.804261Z","relpermalink":"/publication/lan-learning-2019/","section":"publication","summary":"Sequence labeling is a fundamental framework for various natural language processing problems. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios. In many cases, ground truth labels are costly and time-consuming to collect or even non-existent, while imperfect ones could be easily accessed or transferred from different domains. In this paper, we propose a novel framework named consensus Network (ConNet) to conduct training with imperfect annotations from multiple sources. It learns the representation for every weak supervision source and dynamically aggregates them by a context-aware attention mechanism. Finally, it leads to a model reflecting the consensus among multiple sources. We evaluate the proposed framework in two practical settings of multisource learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.","tags":["NER","Crowd Sourcing"],"title":"Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling","type":"publication"},{"authors":["Liyuan Liu","Jingbo Shang","Jiawei Han"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"f446647279da08cdde9c3e965a393569","permalink":"https://liyuanlucasliu.github.io/publication/liu-arabic-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/liu-arabic-2019/","section":"publication","summary":"This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F_1 score of 75.82% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public.","tags":["NER","Arabic"],"title":"Arabic Named Entity Recognition: What Works and What's Next","type":"publication"},{"authors":["Yuning Mao","Liyuan Liu","Qi Zhu","Xiang Ren","Jiawei Han"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"8fea5462d30e0fa16df1a13d5454c2a4","permalink":"https://liyuanlucasliu.github.io/publication/mao-facet-aware-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/mao-facet-aware-2019/","section":"publication","summary":"Commonly adopted metrics for extractive text summarization like ROUGE focus on the lexical similarity and are facet-agnostic. In this paper, we present a facet-aware evaluation procedure for better assessment of the information coverage in extracted summaries while still supporting automatic evaluation once annotated. Specifically, we treat textitfacet instead of textittoken as the basic unit for evaluation, manually annotate the textitsupport sentences for each facet, and directly evaluate extractive methods by comparing the indices of extracted sentences with support sentences. We demonstrate the benefits of the proposed setup by performing a thorough textitquantitative investigation on the CNN/Daily Mail dataset, which in the meantime reveals useful insights of state-of-the-art summarization methods.footnotep̌hantomData can be found at url https://github.com/morningmoni/FAR.","tags":["Evaluation","Extractive Summarization"],"title":"Facet-Aware Evaluation for Extractive Text Summarization","type":"publication"},{"authors":["Liyuan Liu","Zihan Wang","Jingbo Shang","Dandong Yin","Heng Ji","Xiang Ren","Shaowen Wang","Jiawei Han"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"339259da4eb6c8dd7f7ad56f0d337cfe","permalink":"https://liyuanlucasliu.github.io/publication/liu-raw-end-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/liu-raw-end-2019/","section":"publication","summary":"Taking word sequences as the input, typical named entity recognition (NER) models neglect errors from pre-processing (e.g., tokenization). However, these errors can influence the model performance greatly, especially for noisy texts like tweets. Here, we introduce Neural-Char-CRF, a raw-to-end framework that is more robust to pre-processing errors. It takes raw character sequences as inputs and makes end-to-end predictions. Word embedding and contextualized representation models are further tailored to capture textual signals for each character instead of each word. Our model neither requires the conversion from character sequences to word sequences, nor assumes tokenizer can correctly detect all word boundaries. Moreover, we observe our model performance remains unchanged after replacing tokenization with string matching, which demonstrates its potential to be tokenization-free. Extensive experimental results on two public datasets demonstrate the superiority of our proposed method over the state of the art. The implementations and datasets are made available at: https://github.com/LiyuanLucasLiu/Raw-to-End.","tags":["Social Media","NER"],"title":"Raw-to-End Name Entity Recognition in Social Media","type":"publication"},{"authors":["Jingbo Shang","Jiaming Shen","Liyuan Liu","Jiawei Han"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"5ce89b606a8a4f23f385914d605955ec","permalink":"https://liyuanlucasliu.github.io/publication/shang-constructing-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/shang-constructing-2019/","section":"publication","summary":"Real-world data exists largely in the form of unstructured texts. A grand challenge on data mining research is to develop effective and scalable methods that may transform unstructured text into structured knowledge. Based on our vision, it is highly beneficial to transform such text into structured heterogeneous information networks, on which actionable knowledge can be generated based on the user's need. In this tutorial, we provide a comprehensive overview on recent research and development in this direction. First, we introduce a series of effective methods that construct heterogeneous information networks from massive, domain-specific text corpora. Then we discuss methods that mine such text-rich networks based on the user's need. Specifically, we focus on scalable, effective, weakly supervised, language-agnostic methods that work on various kinds of text. We further demonstrate, on real datasets (including news articles, scientific publications, and product reviews), how information networks can be constructed and how they can assist further exploratory analysis.","tags":["Entity Recognition","Network","Phrase","Taxonomy","Tutorial"],"title":"Constructing and Mining Heterogeneous Information Networks from Massive Text","type":"publication"},{"authors":["Ying Lin","Liyuan Liu","Heng Ji","Dong Yu","Jiawei Han"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"a4830e85171916b8097997127d591c2a","permalink":"https://liyuanlucasliu.github.io/publication/lin-reliability-aware-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/lin-reliability-aware-2019/","section":"publication","summary":"Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5% absolute F-score gain.","tags":["NER","Embedding"],"title":"Reliability-aware Dynamic Feature Composition for Name Tagging","type":"publication"},{"authors":["Yujin Yuan","Liyuan Liu","Siliang Tang","Zhongfei Zhang","Yueting Zhuang","Shiliang Pu","Fei Wu","Xiang Ren"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"99827d4354495fe835a97fa69485ba79","permalink":"https://liyuanlucasliu.github.io/publication/yuan-cross-relation-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/yuan-cross-relation-2019/","section":"publication","summary":"Distant supervision leverages knowledge bases to automatically label instances, thus allowing us to train relation extractor without human annotations. However, the generated training data typically contain massive noise, and may result in poor performances with the vanilla supervised learning. In this paper, we propose to conduct multi-instance learning with a novel Cross-relation Cross-bag Selective Attention (C2SA), which leads to noise-robust training for distant supervised relation extractor. Specifically, we employ the sentence-level selective attention to reduce the effect of noisy or mismatched sentences, while the correlation among relations were captured to improve the quality of attention weights. Moreover, instead of treating all entity-pairs equally, we try to pay more attention to entity-pairs with a higher quality. Similarly, we adopt the selective attention mechanism to achieve this goal. Experiments with two types of relation extractor demonstrate the superiority of the proposed approach over the state-of-the-art, while further ablation studies verify our intuitions and demonstrate the effectiveness of our proposed two techniques.","tags":["Relation Extraction","Distant Supervision","Attention"],"title":"Cross-Relation Cross-Bag Attention for Distantly-Supervised Relation Extraction","type":"publication"},{"authors":["Liyuan Liu","Xiang Ren","Jingbo Shang","Jian Peng","Jiawei Han"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"93b61360aafb41c632197411ff54e136","permalink":"https://liyuanlucasliu.github.io/publication/liu-efficient-2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/liu-efficient-2018/","section":"publication","summary":"Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (PTLM), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large models, even in the inference stage, lead to overwhelming computation workloads, thus making them too time-consuming for real-world applications. For a specific task, we aim to keep useful information while compressing bulky PTLM. Since layers of different depths keep different information, we can conduct the compression via layer selection. By introducing the dense connectivity, we can detach any layers without eliminating others, and stretch shallow and wide LMs to be deep and narrow. Moreover, PTLM are trained with layer-wise dropouts for better robustness, and are pruned by a sparse regularization which is customized for our goal. Experiments on benchmarks demonstrate the effectiveness of our proposed method.","tags":["NER","Language Model","Sequence Labeling","Pruning","Selected"],"title":"Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling","type":"publication"},{"authors":["Jingbo Shang*","Liyuan Liu","Xiaotao Gu","Xiang Ren","Teng Ren","Jiawei Han"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"212b06d0f9d0d6ac4bc65590bfafdbce","permalink":"https://liyuanlucasliu.github.io/publication/shang-learning-2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/shang-learning-2018/","section":"publication","summary":"Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.","tags":["NER","Distant Supervision","Selected"],"title":"Learning Named Entity Tagger using Domain-Specific Dictionary","type":"publication"},{"authors":["Huan Gui","Qi Zhu","Liyuan Liu","Aston Zhang","Jiawei Han"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"66cc845edc6b8037819423b022013d07","permalink":"https://liyuanlucasliu.github.io/publication/gui-expert-2018/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/gui-expert-2018/","section":"publication","summary":"Expert finding is an important task in both industry and academia. It is challenging to rank candidates with appropriate expertise for various queries. In addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. We study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content analysis and authority ranking. Regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow semantic meanings. Compared with global embedding learning, locally-trained embedding learning projects the terms into a latent semantic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. Considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of expert finding, provides additional information. Specifically, different types of interactions among objects play different roles. We propose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. To demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. The experiment results show that the proposed framework outperforms existing methods for both general and specific queries.","tags":["Expert Finding","Embedding"],"title":"Expert Finding in Heterogeneous Bibliographic Networks with Locally-trained Embeddings","type":"publication"},{"authors":["Liyuan Liu","Jingbo Shang","Xiang Ren","Frank Fangzheng Xu","Huan Gui","Jian Peng","Jiawei Han"],"categories":null,"content":"","date":1517529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517529600,"objectID":"296723a45e576da01059d0148bcfcf59","permalink":"https://liyuanlucasliu.github.io/publication/liu-empower-2018/","publishdate":"2018-02-02T00:00:00Z","relpermalink":"/publication/liu-empower-2018/","section":"publication","summary":"Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F_1 score of 91.71+/-0.10 without using any extra annotations.","tags":["NER","Language Model","Character-level","Sequence Labeling","Selected"],"title":"Empower Sequence Labeling with Task-Aware Neural Language Model","type":"publication"},{"authors":["Jingbo Shang","Xiyao Shi","Meng Jiang","Liyuan Liu","Timothy Hanratty","Jiawei Han"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"b4ad920568f79b85483992c762ca00c6","permalink":"https://liyuanlucasliu.github.io/publication/shang-contrast-2018/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/shang-contrast-2018/","section":"publication","summary":"Graph pattern mining methods can extract informative and useful patterns from large-scale graphs and capture underlying principles through the overwhelmed information. Contrast analysis serves as a keystone in various fields and has demonstrated its effectiveness in mining valuable information. However, it has been long overlooked in graph pattern mining. Therefore, in this paper, we introduce the concept of contrast subgraph, that is, a subset of nodes that have significantly different edges or edge weights in two given graphs of the same node set. The major challenge comes from the gap between the contrast and the informativeness. Because of the widely existing noise edges in real-world graphs, the contrast may lead to subgraphs of pure noise. To avoid such meaningless subgraphs, we leverage the similarity as the cornerstone of the contrast. Specifically, we first identify a coherent core, which is a small subset of nodes with similar edge structures in the two graphs, and then induce contrast subgraphs from the coherent cores. Moreover, we design a general family of coherence and contrast metrics and derive a polynomial-time algorithm to efficiently extract contrast subgraphs. Extensive experiments verify the necessity of introducing coherent cores as well as the effectiveness and efficiency of our algorithm. Real-world applications demonstrate the tremendous potentials of contrast subgraph mining.","tags":["Contrast","Network"],"title":"Contrast Subgraph Mining from Coherent Cores","type":"publication"},{"authors":["Carl Yang","Mengxiong Liu","Zongyi Wang","Liyuan Liu","Jiawei Han"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"afcd48c0418ebf54e9d48ab2a2d013a4","permalink":"https://liyuanlucasliu.github.io/publication/yang-graph-2017/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/yang-graph-2017/","section":"publication","summary":"Graph clustering (or community detection) has long drawn enormous attention from the research on web mining and information networks. Recent literature on this topic has reached a consensus that node contents and link structures should be integrated for reliable graph clustering, especially in an unsupervised setting. However, existing methods based on shallow models often suffer from content noise and sparsity. In this work, we propose to utilize deep embedding for graph clustering, motivated by the well-recognized power of neural networks in learning intrinsic content representations. Upon that, we capture the dynamic nature of networks through the principle of influence propagation and calculate the dynamic network embedding. Network clusters are then detected based on the stable state of such an embedding. Unlike most existing embedding methods that are task-agnostic, we simultaneously solve for the underlying node representations and the optimal clustering assignments in an end-to-end manner. To provide more insight, we theoretically analyze our interpretation of network clusters and find its underlying connections with two widely applied approaches for network modeling. Extensive experimental results on six real-world datasets including both social networks and citation networks demonstrate the superiority of our proposed model over the state-of-the-art.","tags":["Clustering","Network with Attributes","Propagation","Network"],"title":"Graph Clustering with Dynamic Embedding","type":"publication"},{"authors":["Qi Zhu","Hongwei Ng","Liyuan Liu","Ziwei Ji","Bingjie Jiang","Jiaming Shen","Huan Gui"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"d1ff3abbe39bf05cfedd7913cd7d980c","permalink":"https://liyuanlucasliu.github.io/publication/zhu-wikidata-2017/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/zhu-wikidata-2017/","section":"publication","summary":"Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation. As it can be edited by anyone, entries frequently get vandalized, leading to the possibility that it might spread of falsified information if such posts are not detected. The WSDM 2017 Wiki Vandalism Detection Challenge requires us to solve this problem by computing a vandalism score denoting the likelihood that a revision corresponds to an act of vandalism and performance is measured using the ROC-AUC obtained on a held-out test set. This paper provides the details of our submission that obtained an ROC-AUC score of 0.91976 in the final evaluation.","tags":["Vandalism Detection","WSDM Cup"],"title":"Wikidata Vandalism Detection - The Loganberry Vandalism Detector at WSDM Cup 2017","type":"publication"},{"authors":["Liyuan Liu","Xiang Ren*","Qi Zhu","Shi Zhi","Huan Gui","Heng Ji","Jiawei Han"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"5498dbb311a27384e32550c517a52187","permalink":"https://liyuanlucasliu.github.io/publication/liu-heterogeneous-2017/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/liu-heterogeneous-2017/","section":"publication","summary":"Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.","tags":["Heterogeneous Supervision","Relation Extraction","Distant Supervision","Selected"],"title":"Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach","type":"publication"},{"authors":["Chao Zhang","Liyuan Liu","Dongming Lei","Quan Yuan","Honglei Zhuang","Tim Hanratty","Jiawei Han"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"38e906261fcb3f73164e5211cf55188a","permalink":"https://liyuanlucasliu.github.io/publication/zhang-triovecevent-2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/zhang-triovecevent-2017/","section":"publication","summary":"Detecting local events (e.g., protest, disaster) at their onsets is an important task for a wide spectrum of applications, ranging from disaster control to crime monitoring and place recommendation. Recent years have witnessed growing interest in leveraging geo-tagged tweet streams for online local event detection. Nevertheless, the accuracies of existing methods still remain unsatisfactory for building reliable local event detection systems. We propose TRIOVECEVENT, a method that leverages multimodal embeddings to achieve accurate online local event detection. The effectiveness of TRIOVECEVENT is underpinned by its two-step detection scheme. First, it ensures a high coverage of the underlying local events by dividing the tweets in the query window into coherent geo-topic clusters. To generate quality geo-topic clusters, we capture short-text semantics by learning multimodal embeddings of the location, time, and text, and then perform online clustering with a novel Bayesian mixture model. Second, TRIOVECEVENT considers the geo-topic clusters as candidate events and extracts a set of features for classifying the candidates. Leveraging the multimodal embeddings as background knowledge, we introduce discriminative features that can well characterize local events, which enable pinpointing true local events from the candidate pool with a small amount of training data. We have used crowdsourcing to evaluate TRIOVECEVENT, and found that it improves the performance of the state-of-the-art method by a large margin.","tags":["Social Media","Clustering","Event Detection","Embedding"],"title":"TrioVecEvent: Embedding-based Online Local Event Detection in Geo-tagged Tweet Streams","type":"publication"},{"authors":["Liyuan Liu","Linli Xu","Zhen Wang","Enhong Chen"],"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"3c23229bc4275bb7490ec06310a9da46","permalink":"https://liyuanlucasliu.github.io/publication/liu-community-2015/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/liu-community-2015/","section":"publication","summary":"With the recent advances in information networks, the problem of identifying group structure or communities has received a significant amount of attention. Most of the existing principles of community detection or clustering mainly focus on either the topological structure of a network or the node attributes separately, while both of the two aspects provide valuable information to characterize the nature of communities. In this paper we combine the topological structure of a network as well as the content information of nodes in the task of detecting communities in information networks. Specifically, we treat a network as a dynamic system and consider its community structure as a consequence of interactions among nodes. To model the interactions we introduce the principle of content propagation and integrate the aspects of structure and content in a network naturally. We further describe the interactions among nodes in two different ways, including a linear model to approximate influence propagation, and modeling the interactions directly with random walk. Based on interaction modeling, the nature of communities is described by analyzing the stable status of the dynamic system. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed framework over the state of the art.","tags":["Clustering","Propagation"],"title":"Community Detection Based on Structure and Content: A Content Propagation Perspective","type":"publication"}]