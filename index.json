[{"authors":["liyuan-liu"],"categories":null,"content":"Welcome to Lucas Liyuan Liu (刘力源)\u0026lsquo;s webpage! I am a Ph.D. student at the University of Illinois at Urbana-Champaign (UIUC), advised by Prof. Jiawei Han. I received B.Eng. from the University of Science and Technology of China (USTC), majoring in Computer Science. My undergraduate advisor is Prof. Linli Xu.\nIf you are going to visit UIUC, please let me buy you a bubble tea . PS: House Milk Tea and Black Sugar Milk Tea (w. no additional sugar) are pretty good.\n","date":1601510400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1601510400,"objectID":"5329f68875b61a2fd3f27d141036c39a","permalink":"https://liyuanlucasliu.github.io/authors/liyuan-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liyuan-liu/","section":"authors","summary":"Welcome to Lucas Liyuan Liu (刘力源)\u0026lsquo;s webpage! I am a Ph.D. student at the University of Illinois at Urbana-Champaign (UIUC), advised by Prof. Jiawei Han. I received B.Eng. from the University of Science and Technology of China (USTC), majoring in Computer Science. My undergraduate advisor is Prof. Linli Xu.\nIf you are going to visit UIUC, please let me buy you a bubble tea . PS: House Milk Tea and Black Sugar Milk Tea (w.","tags":null,"title":"Liyuan Liu","type":"authors"},{"authors":null,"categories":null,"content":" What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding  Position encoding learns local position information in the encoder and absolute position in the decoder.   A Matter of Framing: The Impact of Linguistic Formalism on Probing Results  Choice of linguistic formalism is important for role-semantic probing results.   Contrastive Distillation on Intermediate Representations for Language Model Compression  Language model compression via contrastive loss, while memory bank is used to store negative samples (not updated in training).   Efficient Meta Lifelong-Learning with Limited Memory  Efficient experience rehearsal is achieved by learn to select examples.   Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks  Introduced optimization-based meta-learning to language modeling pretraining-finetuning paradigm.   Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference  Multi-head attention can be viewed as multiple sample from the same distribution.   Token-level Adaptive Training for Neural Machine Translation  Using frequency-based heuristic to re-weight the loss value for different tokens.   Shallow-to-Deep Training for Neural Machine Translation  Both copying-strategy and lr-reset matters for progressive Transformer-based NMT training (copying is more important and 123-\u0026gt;123123 \u0026gt; 123-\u0026gt;112233 \u0026gt; 123-\u0026gt;123333)   Incorporating a Local Translation Mechanism into Non-autoregressive Translation  In NAT, treating an autoregressive sequence instead of a token as the basic unit (for 1234 generate 12 and 34 simultaneously while 2 is generated after 1)   Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation  To alleviate error accumulation, use attention mask to change classical attention (global, all tokens are accessible) to local attention.   Masking as an Efficient Alternative to Finetuning for Pretrained Language Models  Instead of finetuning LMs, learn a weight mask instead (e.g., [0.1, 0.3] -\u0026gt; [0.0, 0.3]), which is argued to be more memory-efficient (1-bit instead of 32/16-bit additional copy).   If Beam Search is the Answer, What was the Question?  Beam search has an inductive bias which can be linked to the promotion of uniform information density \u0026mdash; variance of surprisals is linear to BLEU (people dont want surprise in reading). By adding regularization on information density, exact search performs similar to beam search.   When BERT Plays the Lottery, All Tickets Are Winning  The pruned “good” subnetworks works well, while the “bad” ones do not. For structured pruning, even “bad” subnetworks can be finetuned separately to reach fairly strong performance. Different runs get different “good” subnetworks, indicating the existance of factors otherthan non-trivial linguistic features. The success of BERT might be more related to optimization surfaces rather than specific bits of linguistic knowledge.   Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning  Using RL to select context for NMT at the document-level.   Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation  Pruning in training is much better than pruning after converge (0.1 BLEU drop v.s. 1.1)   Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT  Pre-training on high-resource -\u0026gt; fine-tune on low-resource by expanding dictionary -\u0026gt; XLM   Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems  Char-level NMT \u0026lt; first train Subword-level NMT, then convert to Character-level \u0026lt; Subword-level NMT   Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation  Jointly learn to select training data from comparable (rather than parallel) data, and to NMT.   Can Automatic Post-Editing Improve NMT?  With more training data, automatic post-editing can improve NMT a lot.   Learning from Context or Names? An Empirical Study on Neural Relation Extraction  Both context and names are important for performance, while existing RE datasets may leak cues via names.   Some Languages Seem Easier to Parse Because Their Treebanks Leak  Some languages seem easier to parse, since the trees in the test set are mostly isomorphic with some tree in the training set.   Word Frequency Does Not Predict Grammatical Knowledge in Language Models  Comparing The cat walks and The cat walk, the author found in four dimensions, frequency is not related to how well the knowledge is learned.   Do sequence-to-sequence VAEs learn global features of sentences?  VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness, while chaning architecture can help to alleviate such memorization.   Adversarial Semantic Collisions  Semantic collisions refer to texts that are semantically unrelated but judged as similar by NLP models, which can be effectively derived with gradient based methods.   Sparse Text Generation  Entmax transformation is leveraged to train and sample from a natively sparse language model, while \\alpha-Entmax unifies softmax, argmax, and sparsemax.   Learning VariationalWord Masks to Improve the Interpretability of Neural Text Classifiers  word masks are automatically learned to emphasize task-specific words, which improves interpretability and performance.   Identifying Elements Essential for BERT’s Multilinguality  shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality. Word order is relevant: BERT is not multilingual with one language having an inverted word order. The comparability of training corpora contributes to multilinguality.   A Streaming Approach For Efficient Batched Beam Search  Periodically “refills” the batch before proceeding with a selected subset of candidates, special handlings are made to handle the self-attention padding.   On Losses for Modern Language Models  Clarify NSP’s effect on BERT pre-training and design more auxiliary tasks, which allows the final method outperforms BERTBase with fewer than a quarter of the training tokens.   Entities as Experts: Sparse Memory Access with Entity Supervision  Leveraging entity representations to better memorize sparse knowledge.   Semantic Label Smoothing for Sequence to Sequence Problems  Retrieve semantically similar sentences to do label smoothing, at the cost of training computations.   We Can Detect Your Bias: Predicting the Political Ideology of News Articles  Directly using BERT would result in a heavy dependency on source, which motivates the leverage of adversarial training to neutralize such leaked clues.    ","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"b3ab4404131c6ec7354f6bc072a29636","permalink":"https://liyuanlucasliu.github.io/blog/2020-11-emnlp/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/blog/2020-11-emnlp/","section":"blog","summary":"One sentence summary for each paper I read during EMNLP 2020.","tags":["EMNLP","Paper Summary"],"title":"One Sentence Summary for EMNLP 2020","type":"blog"},{"authors":["Xiaotao Gu","Liyuan Liu","Hongkun Yu","Jing Li","Chen Chen","Jiawei Han"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"966b0f242fb7ea4ebad19ae9c332d173","permalink":"https://liyuanlucasliu.github.io/publication/gu-progressive-2020/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/gu-progressive-2020/","section":"publication","summary":"As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively--start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and large models respectively while achieving comparable performances. Code will be released for reproduction and future studies.","tags":["Bert","Network Grow","Adaptive"],"title":"On the Transformer Growth for Progressive BERT Training","type":"publication"},{"authors":["Zichao Li*","Liyuan Liu","Chengyu Dong","Jingbo Shang"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"75f361edcf42ceec2b63397062fb36ad","permalink":"https://liyuanlucasliu.github.io/publication/li-apart-2020/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/li-apart-2020/","section":"publication","summary":"Our goal is to understand why the robustness drops after conducting adversarial training for too long. Although this phenomenon is commonly explained as overfitting, our analysis suggest that its primary cause is perturbation underfitting. We observe that after training for too long, FGSM-generated perturbations deteriorate into random noise. Intuitively, since no parameter updates are made to strengthen the perturbation generator, once this process collapses, it could be trapped in such local optima. Also, sophisticating this process could mostly avoid the robustness drop, which supports that this phenomenon is caused by underfitting instead of overfitting. In the light of our analyses, we propose APART, an adaptive adversarial training framework, which parameterizes perturbation generation and progressively strengthens them. Shielding perturbations from underfitting unleashes the potential of our framework. In our experiments, APART provides comparable or even better robustness than PGD-10, with only about 1/4 of its computational cost.","tags":["Adversarial Training","Stability","Selected"],"title":"Overfitting or Underfitting? Understand Robustness Drop in Adversarial Training","type":"publication"},{"authors":null,"categories":null,"content":" Chairs:  I purchased a refurbished leap v2 chair from madison seating. Its website has some horrible reviews, but its ebay shop seems legit. Still, for refurbished chairs, they are used and YMMV. My girlfriend is using a chair from Autonomous, it is also great. This compant sells some fully functioning seats in a relative low price (although I cannot use chrome to open their homepage on Windows\u0026hellip;).   Desk:  Big NO to rising desks (or adjustable height desks). Although I love these desks, the dealbreaker is that, sooner or later, its motors are gonna break, which is super troublesome. Easy and reliable alternatives are the furniture rise + cheapest IKEA table combination.   Monitor stand:  Monitor stands are awesome! Save lots of desk space and greatly simplifies the workspace. Note that if you are using 32\u0026rsquo;\u0026rsquo; monitor/heavy monitor, be sure to buy monitor stand with a heavy load (only the ones with gas spring arms can hold my monitor). I also attach headphone mounts to my monitor. They look great! I also mount my laptop on my monitor stand with an additional spring arm\u0026ndash; VESA laptop mount tray works great.   Mouse and Keyboards:  Logitech MX Master is the only mouse I would recommend (v1, v2, v3 are all great). Any of the following keyboards are great:  Microsoft Ergonomic Keyboard (cheapest, wired and slow actuation) Microsoft Sculpt Ergonomic Keyboard (wireless, not bluetooth enabled, using batteries and fast actuation) Logitech MX Keys (not ergonomic, wireless, bluetooth enabled, rechargeable and fast actuation) Logitech K860 (wireless, bluetooth enabled, using batteries and fast actuation)      ","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"c6e235096082c9c1e367e5832e5420f3","permalink":"https://liyuanlucasliu.github.io/blog/2020-08-wfh/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/blog/2020-08-wfh/","section":"blog","summary":"The Ultimate Setup for Working From Home.","tags":["DIY"],"title":"My Work-From-Home Secrets","type":"blog"},{"authors":["Xiaodong Liu","Kevin Duh","Liyuan Liu","Jianfeng Gao"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"8805a97bb887c950200fcc9504216c8f","permalink":"https://liyuanlucasliu.github.io/publication/liu-deep-2020/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/liu-deep-2020/","section":"publication","summary":"We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: https://github.com/namisan/exdeep-nmt.","tags":["Transformer","Network Depth","NMT"],"title":"Very Deep Transformers for Neural Machine Translation","type":"publication"},{"authors":["Chengyu Dong","Liyuan Liu","Zichao Li","Jingbo Shang"],"categories":null,"content":"","date":1593734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593734400,"objectID":"97a7389dde1c97471fd9223d1938bfaa","permalink":"https://liyuanlucasliu.github.io/publication/dong-lipgrow-2020/","publishdate":"2020-07-03T00:00:00Z","relpermalink":"/publication/dong-lipgrow-2020/","section":"publication","summary":"In pursuit of resource-economical machine learning, attempts have been made to dynamically adjust computation workloads in different training stages, i.e., starting with a shallow network and gradually increasing the model depth (and computation workloads) during training. However, there is neither guarantee nor guidance on designing such network grow, due to the lack of its theoretical underpinnings. In this work, to explore the theory behind, we conduct theoretical analyses from an ordinary differential equation perspective. Specifically, we illustrate the dynamics of network growth and propose a novel performance measure specific to the depth increase. Illuminated by our analyses, we move towards theoretically sound growing operations and schedulers, giving rise to an adaptive training algorithm for residual networks, LipGrow, which automatically increases network depth thus accelerates training. In our experiments, it achieves comparable performance while reducing ∼50% of training time.","tags":["Network Grow","Neural-ODE","Network Depth","Selected","Adaptive"],"title":"Towards Adaptive Residual Network Training: A Neural-ODE Perspective","type":"publication"},{"authors":["Yuning Mao","Liyuan Liu","Qi Zhu","Xiang Ren","Jiawei Han"],"categories":null,"content":"","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648000,"objectID":"8fea5462d30e0fa16df1a13d5454c2a4","permalink":"https://liyuanlucasliu.github.io/publication/mao-facet-aware-2019/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/publication/mao-facet-aware-2019/","section":"publication","summary":"Commonly adopted metrics for extractive text summarization like ROUGE focus on the lexical similarity and are facet-agnostic. In this paper, we present a facet-aware evaluation procedure for better assessment of the information coverage in extracted summaries while still supporting automatic evaluation once annotated. Specifically, we treat facet instead of token as the basic unit for evaluation, manually annotate the support sentences for each facet, and directly evaluate extractive methods by comparing the indices of extracted sentences with support sentences. We demonstrate the benefits of the proposed setup by performing a thorough quantitative investigation on the CNN/Daily Mail dataset, which in the meantime reveals useful insights of state-of-the-art summarization methods. Data can be found at https://github.com/morningmoni/FAR.","tags":["Evaluation","Extractive Summarization"],"title":"Facet-Aware Evaluation for Extractive Text Summarization","type":"publication"},{"authors":["Ouyu Lan","Xiao Huang","Bill Yuchen Lin","He Jiang","Liyuan Liu","Xiang Ren"],"categories":null,"content":"","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648000,"objectID":"4fbbc0669f309539ba731a88b00fbe83","permalink":"https://liyuanlucasliu.github.io/publication/lan-learning-2019/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/publication/lan-learning-2019/","section":"publication","summary":"Sequence labeling is a fundamental framework for various natural language processing problems. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios, and obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data...). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders.","tags":["NER","Crowd Sourcing"],"title":"Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling","type":"publication"},{"authors":null,"categories":null,"content":" Docker  Nvidia Image: TensorRT, PyTorch  docker pull nvcr.io/nvidia/tensorrt:20.03-py3 docker pull nvcr.io/nvidia/pytorch:20.03-py3   Commands:  remove image: docker rmi $name prune: docker system prune   Permission Error:  fixing by sudo chmod 666 /var/run/docker.sock   No enough space:  moving to a different disk by sudo cp -r /var/lib/docker $TARGET_PATH echo '{ \u0026quot;graph\u0026quot;: \u0026quot;$TARGET_PATH\u0026quot; }' | sudo tee -a /etc/docker/daemon.json sudo systemctl daemon-reload sudo systemctl restart docker sudo chmod 666 /var/run/docker.sock       Jupyter   Jupyter Extensions  conda install -c conda-forge jupyter_contrib_nbextensions     Windows  Terminal: Termius (education account w. github student) Windows Subsystem for Linux:  install:  run: Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform in powershell w. admin run: Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux in powershell w. admin get Ubuntu from microsoft store \u0026amp; RUN the app (Ubuntu refers to the recommended version, should be 20.04) run: wsl --set-default-version 2 Now, the ubuntu\u0026rsquo;s bash.exe is available:  windows file system is mounted at /mnt/     anyconnect not working in WSL:  use anyconnect within the windows store. reference     Windows Terminal:  Configuration is saved at here   Multi-Monitor:  Enable multi task bar (search taskbar) Make the larger one major monitor and neven group icons (search display) Only show the icon of searching (right click at taskbar-\u0026gt;search) Hide cortana and task buttons by right click at taskbar.   Win-PE creator:  http://www.wepe.com.cn/     Sublime:  install package control w install package control; theme: default (or ayu) sftp: for windows: convert ssh to ppk with puttygen in https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html.  setting: \u0026quot;ssh_key_file\u0026quot;: \u0026quot;C:/Users/Liyuan Liu - LL/.ssh/id_rsa.ppk\u0026quot;   remote editing alternative: https://github.com/textmate/rmate sidebarenhancement   Tmux   oh my tmux tmux 1.8 config:  https://github.com/mandre/dotfiles/blob/master/.tmux-1.8.conf   save and restore tmux section:   tmux-plugin-manager  tmux-resurrent   save pane content by commands  command to use in tmux: tmux capture-pane -pS -1000000 \u0026gt; file.out     bash commands  generate  basic: ssh-keygen advanced: ssh-keygen -t idrsa -b 4096 -C \u0026quot;llychinalz@gmail.com\u0026quot;   authorize server login w. ssh  recommended:ssh-copy-id -i idrsa $remote_username@$server_ip_address -p $port   add ssh key: eval `ssh-agent` ssh-add idrsa     linux  check linux version: uname -m \u0026amp;\u0026amp; cat /etc/*release check cuda version:cat /usr/local/cuda/version.txt output gpu utils to one line:nvidia-smi --query-gpu=memory.used --format=csv,noheader | xargs | sed -e 's/ /,/g' autojump install sudo apt-get install autojump echo \u0026quot;. /usr/share/autojump/autojump.sh\u0026quot; | tee -a ~/.zshrc   zsh  oh-my-zsh sh -c \u0026quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026quot; zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions plug-in permissions compaudit | xargs chmod g-w,o-w on WSL, the git plut-in may be too slow, turn off by git config --global oh-my-zsh.hide-status 1     NAS  rrshare with updated API   docker image and readme     Nvidia Geforce GPU  restrict base clock and temperature to protect cards:  https://zhuanlan.zhihu.com/p/164907210     PyTorch  solution for Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL...:  python: os.environ['MKL_THREADING_LAYER'] = 'GNU' command: MKL_THREADING_LAYER=GNU python my_script.py env: export MKL_THREADING_LAYER=GNU     RStudio  docker run -d --name rstudio -v $HOME:/home/rstudio -e PASSWORD=$PASSWORD -p 8787:8787 rocker/tidyverse   Markdown editor  Joplin + Typora   Printer   docker repo source stop cups service for DSM synoservice --hard-disable cupsd synoservice --hard-disable cups-lpd synoservicectl --stop cupsd synoservicectl --stop cups-lpd synoservicecfg --hard-disable cupsd synoservicecfg --hard-disable cups-lpd   command to start: docker run -d --name=\u0026quot;airprint\u0026quot; --net=\u0026quot;host\u0026quot; --privileged=\u0026quot;true\u0026quot; \\ -e \u0026quot;CUPS_USER_ADMIN\u0026quot;=$USER -e \u0026quot;CUPS_USER_PASSWORD\u0026quot;=$PASS \\ -v $PATH_TO_AIRPRINT_CONFIG:/config \\ -v /dev/bus/usb:/dev/bus/usb \\ yaurora/cups-google-airprint   Login and install Canon DFRII driver: docker exec -it airprint /bin/bash wget http://gdlp01.c-wss.com/gds/6/0100009236/06/linux-UFRII-drv-v510-usen-09.tar.gz tar -xzf linux-UFRII-drv-v510-usen-09.tar.gz cd linux-UFRII-drv-v510-usen ./install.sh   Network address for windows: http://$IP/printers/Canon_D530_D560      ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"24fa74fee187d63d712cdbc2fdd06b0a","permalink":"https://liyuanlucasliu.github.io/blog/2020-07-setup/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/blog/2020-07-setup/","section":"blog","summary":"Cheatsheet for Common Configurations.","tags":["DIY"],"title":"Configuration Cheatsheet","type":"blog"},{"authors":["Honglei Zhuang","Fang Guo","Chao Zhang","Liyuan Liu","Jiawei Han"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"9fa92dfc660a2f0e5a18bd8c01528551","permalink":"https://liyuanlucasliu.github.io/publication/zhuang-sentiment-2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/zhuang-sentiment-2020/","section":"publication","summary":"Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.","tags":["Sentiment Analysis","Minimal User Guidance"],"title":"Joint Aspect-Sentiment Analysis with Minimal User Guidance","type":"publication"},{"authors":["Liyuan Liu","Xiaodong Liu","Jianfeng Gao","Weizhu Chen","Jiawei Han"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d4955811625380afdd5ceac284d90371","permalink":"https://liyuanlucasliu.github.io/publication/liu-admin-2020/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/liu-admin-2020/","section":"publication","summary":"Transformers have been proved effective for many deep learning tasks. Training transformers, however, requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (the standard SGD fails to train Transformers effectively). In this paper, we study Transformer training from both theoretical and empirical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that substantially influences training. Specifically, we observe that for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable since it amplifies small parameter perturbations (e.g., parameter updates) and result in significant disturbances in the model output, yet a light dependency limits the potential of model training and can lead to an inferior trained model. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the training in the early stage and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance.","tags":["Transformer","Initialization","Stability","Selected"],"title":"Understanding the Difficulty of Training Transformers","type":"publication"},{"authors":null,"categories":null,"content":"For recent Nvidia GPUs like V100 or R8000, allowing half-precision training can get roughly 2X speedup instantly, and the only problem is how to conduct mixed-precision training?\nUnlike TPU, typical Nvidia GPUs follow the IEEE fp16 standard instead of bfp16. Specifically, TPU uses the bfp16 standard with 8 exponent bits and 7 mantissa bits, while fp16 has only 5 exponent bits and 10 mantissa bits. With less exponent bits, directly applying half-precision to deep learning leads to tons of overflow and underflow.\nTo compensate the loss of exponent bits, dynamic loss scaling and mixed-precision training have been proposed. The fairseq package comes with naive fp16 support, and for custom models and other PyTorch codebases, the Apex package is usually the go-to choice. Besides the apex documentation, below provides some tips for mixed-precision training.\nDynamic Loss Scaling  TL;DL. Since it is easier to detect overflow then under, a minimal loss scale is recommended to set (e.g., 0.03125) and a small window helps to stabilize the training (e.g., 256).\n Unlike underflow, the overflow can be easily detected. Accordingly, it is strategically benefical to make the model overflow to avoid underflow. Specifically, loss scaling is proposed to first scale-up the loss and gradients by a constant, then divide the gradient by the same constant after back-propagation. Still, the choice of the constant is an open problem. Dynamic loss scaling is leveraged to dynamically update this constant during training. Specifically, it starts from setting the constant to a large value, then halve its value when an overflow occurs, which indicates the constant value is probably too large. Also, if no overflow is detected in N continuous updates which indicates the constant could be larger, it duplicates the constant.\nMixed Precision Training  TL;DL. Elementwise_affine parameters are easier to overflow and it is helpful to cast this part to fp32.\n Others  In apex, opt_level can be set to O0 (full fp32), O1 (mixed precision), O2 (almost fp16), and O3 (full fp16). To specifically cast a model to fp32:  set model parameters, e.g., for n, p in model.named_parameters(): if any([ki in n for ki in fp32_keys]): p.float()    cast precision conversion by monkey patching, e.g., orig_linear = torch.nn.functional.linear def wrapped_linear(*args): casted_args = [] for arg in args: if torch.is_tensor(arg) and torch.is_floating_point(arg): casted_args.append(arg.float()) else: casted_args.append(arg) return orig_linear(*casted_args) torch.nn.functional.linear = wrapped_linear       ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"9c550464b9faed5b2524196e464746a4","permalink":"https://liyuanlucasliu.github.io/blog/2020-03-fp16/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/blog/2020-03-fp16/","section":"blog","summary":"How to Conduct Mixed-Precision Training with GPUs","tags":["FP16","GPU","PyTorch"],"title":"FP16 and Apex","type":"blog"},{"authors":null,"categories":null,"content":"I found NextCloud to be very powerful, flexible and expandable. Specifically, I setup NextCloud on my synology NAS via docker, manually upload my file (while will be essentially faster) for the initial sync, then use NextCloud as a combination of Dropbox and Google Drive.\nSetup  Setup a new network for the docker; Setup path as a folder to store data; Using the following docker compose file to build docker images (sensitive information is masked in \u0026lt; and \u0026gt;). The command for set up is docker-compose up -d, and the command for update is sudo docker-compose pull \u0026amp;\u0026amp; sudo docker-compose up -d; Login with setting database to mariadb; After manually uploading files to the destination, you can add files to nextcloud by sudo -u www-data php occ files:scan --path \u0026quot;\u0026lt;username\u0026gt;/files/\u0026lt;path_under_files\u0026gt;; To recursively delete, use find . -type d -name '@eaDir' -exec rm -r {} +; Remember to add trusted domains.  version: '3.2' services: nextcloud: image: nextcloud container_name: nextcloud restart: always networks: - net ports: - 3222:80 labels: - \u0026quot;traefik.docker.network=net\u0026quot; - \u0026quot;traefik.enable=true\u0026quot; - \u0026quot;traefik.backend=nextcloud\u0026quot; - \u0026quot;traefik.port=80\u0026quot; - \u0026quot;traefik.protocol=http\u0026quot; - \u0026quot;traefik.frontend.rule=Host:\u0026lt;domain name\u0026gt;\u0026quot; - \u0026quot;traefik.frontend.headers.customResponseHeaders=Strict-Transport-Security:15552000\u0026quot; - \u0026quot;traefik.frontend.passHostHeader=true\u0026quot; - \u0026quot;traefik.frontend.redirect.permanent:true\u0026quot; - \u0026quot;traefik.frontend.redirect.regex:https://(.*)/.well-known/(card|cal)dav\u0026quot; - \u0026quot;traefik.frontend.redirect.replacement:https://$$1/remote.php/dav/\u0026quot; depends_on: - nextcloud_db volumes: - \u0026lt;path\u0026gt;/nextcloud:/var/www/html - \u0026lt;path\u0026gt;/config:/var/www/html/config - \u0026lt;path\u0026gt;/data:/var/www/html/data - \u0026quot;MYSQL_HOST=nextcloud_db\u0026quot; - \u0026quot;NEXTCLOUD_TRUSTED_DOMAINS=\u0026lt;domain\u0026gt;\u0026quot; - \u0026quot;REDIS_HOST=nextcloud_redis\u0026quot; depends_on: - nextcloud_db - redis nextcloud_db: image: mariadb container_name: nextcloud_db command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW restart: always networks: - net volumes: - \u0026lt;path\u0026gt;/db:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=\u0026lt;password\u0026gt; - MYSQL_PASSWORD=\u0026lt;password\u0026gt; - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud redis: image: redis container_name: nextcloud_redis networks: - net cron: image: nextcloud container_name: nextcloud_cron restart: always networks: - net volumes: - \u0026lt;path\u0026gt;/nextcloud:/var/www/html entrypoint: /cron.sh depends_on: - nextcloud_db - redis volumes: nextcloud: nextcloud_db: nextcloud_cron: networks: net: external: true  ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"8728b616f42bc8cec509e71a07dff784","permalink":"https://liyuanlucasliu.github.io/blog/2020-03-nextcloud/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/blog/2020-03-nextcloud/","section":"blog","summary":"Can I host some cloud services on my own server?","tags":["DIY","NAS","Cloud"],"title":"NextCloud Docker Setup","type":"blog"},{"authors":null,"categories":null,"content":"Last night, my Macbook Pro 2015 broke from an unsuccessful program install and could not start up. Time machine saves me, again.\nData is priceless and duplication is the only way to protect it. Time Machine is a useful program, however, its speed could be deadly slow if you are using a remote storage (instead of directly connected via usb or other cable).\nAfter many trial and error attempts, my current solution is:\n use NextCloud to sync codes, images, and other non-application non-system files. If you are also a Synology user, this platform is much better than Drive 2.0; use Time Machine to backup only system files and applications, with the speed-up setting as below (use the SMB protocol instead of the AFP protocol for Synology).  Even only a small portion of my files (~100 Gb) is handled by Time Machine, they could be a very slow 100 Gb without speeding up. The reason is the system sets an upper bound on the performance of Time Machine by default. Running the following code would save you tons of time:\nsudo sysctl debug.lowpri_throttle_enabled=0  You can change it back after the first \u0026amp; major backups with:\nsudo sysctl debug.lowpri_throttle_enabled=1  ","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582934400,"objectID":"5117b479586eb67cc94e2c5e513da0ca","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-tm/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/blog/2020-02-tm/","section":"blog","summary":"Time Machine, why are you sooo slooow and how to speed you up.","tags":["DIY","NAS","Time Machine"],"title":"How to Save You (Make You Fast), Time Machine","type":"blog"},{"authors":null,"categories":null,"content":"After reaching the 2 Gb free space limit of Mendeley, I did some research and decide to use Zotero as my pdf management tool. Different from Mendeley, Zotero supports WebDAV-based sync.\nHowever, importing pdfs from Mendelay from Zotero is not easy, since the latest Mendelay release encrypts its database. Below is some notes on how to hack the importing.\n sync Mendelay to the cloud and download all pdfs. delete the Mendelay library and app, move downloaded pdfs to a different folder; download an old version Mendelay, sync from the cloud, and add pdfs from the previous folder; export to zotero  ","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582675200,"objectID":"b0546a321a360b7c81fbb9cb27a328be","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-zotero/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/blog/2020-02-zotero/","section":"blog","summary":"How to Importing Documents from Mendeley (Un-official Tricks).","tags":["DIY","NAS","Zotero"],"title":"Zotero, Importing from Mendeley","type":"blog"},{"authors":null,"categories":null,"content":"Since I need a router to turn the printer (gift from Jingbo) into a internet printer, I did some research on cheap-but-powerful routers.\nK3C costs about 30$ but performs like a 200$ router.\nUpgrading to the un-official firearm is highly recommended, which unlocks tons of features, including Ads-blocking, USB-printer, etc.\n A nice tutorial is available in Chinese. However, no available tutorial is for K3C-international (i.e., can be brought from U.S.). After some tiny-little explorations, I successfully finished the upgrade. Here are some notes based on my experience:\n upgrading to unofficial versions is risky; all operations are at your own risk; the version number of international-K3C firearm is in a different serial (from the tutorial), so ignore all version information in the tutorial; update the official firearm to the latest version, and then do the upgrade (I did this and it worked for me); after upgrade to unofficial 1.1, please go ahead and upgrade to unofficial 1.6 (1.7 is not very stable and not recommended); you need a windows pc for the upgrade\u0026hellip; (based on the tutorial); some links in the tutorial page is outdated, please find the files as below:  Necessary files:\n Telnet Tool V1.1 Firearm V1.6 Firearm  ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"05c1be7b3ee93d34e7ccb52d157fc230","permalink":"https://liyuanlucasliu.github.io/blog/2020-02-k3c/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/blog/2020-02-k3c/","section":"blog","summary":"How to upgrade the firearm to the un-official version for K3C-international.","tags":["DIY","Router"],"title":"Hands Down, Best Router within 100$: K3C (which costs 30 USD)","type":"blog"},{"authors":["Jingbo Shang*","Xinyang Zhang*","Liyuan Liu","Sha Li","Jiawei Han"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"a79fac521f6a13a97fe0b63da9aa8c4b","permalink":"https://liyuanlucasliu.github.io/publication/shang-nettaxo-2020/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/shang-nettaxo-2020/","section":"publication","summary":"The automated construction of topic taxonomies can benefit numerous applications, including web search, recommendation, and knowledge discovery. One of the major advantages of automatic taxonomy construction is the ability to capture corpus-specific information and adapt to different scenarios. To better reflect the characteristics of a corpus, we take the meta-data of documents into consideration and view the corpus as a text-rich network. In this paper, we propose NetTaxo, a novel automatic topic taxonomy construction framework, which goes beyond the existing paradigm and allows text data to collaborate with network structure. Specifically, we learn term embeddings from both text and network as contexts. Network motifs are adopted to capture appropriate network contexts. We conduct an instance-level selection for motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node. Extensive experiments on two real-world datasets demonstrate the superiority of our method over the state-of-the-art, and further verify the effectiveness and importance of instance-level motif selection.","tags":["Taxonomies","Network"],"title":"NetTaxo: Automated Topic Taxonomy Construction from Large-Scale Text-Rich Network","type":"publication"},{"authors":["Liyuan Liu","Haoming Jiang","Pengcheng He","Weizhu Chen","Xiaodong Liu","Jianfeng Gao","Jiawei Han"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"3c6978df7609dc44500a46bcad7d634f","permalink":"https://liyuanlucasliu.github.io/publication/liu-radam-2020/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/liu-radam-2020/","section":"publication","summary":"The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.","tags":["Warmup","Optimization","Selected"],"title":"On the Variance of the Adaptive Learning Rate and Beyond","type":"publication"},{"authors":["Zihan Wang*","Jingbo Shang*","Liyuan Liu","Lihao Lu","Jiacheng Liu","Jiawei Han"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bbe5da5bfc94a68b2fcc0e1ec4f0ff6f","permalink":"https://liyuanlucasliu.github.io/publication/wang-crossweigh-2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/wang-crossweigh-2019/","section":"publication","summary":"Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh.","tags":["NER","Label Mistake","CoNLL03","Selected","Evaluation"],"title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations","type":"publication"},{"authors":["Qinyuan Ye*","Liyuan Liu","Maosen Zhang","Xiang Ren"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d7535dc1789bf45d9c296bcb48fc53ad","permalink":"https://liyuanlucasliu.github.io/publication/ye-looking-2019/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/ye-looking-2019/","section":"publication","summary":"In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23% relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.","tags":["Shifted Label Distribution","Relation Extraction","Distant Supervision","Selected"],"title":"Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Liyuan Liu","Jingbo Shang","Jiawei Han"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"f446647279da08cdde9c3e965a393569","permalink":"https://liyuanlucasliu.github.io/publication/liu-arabic-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/liu-arabic-2019/","section":"publication","summary":"This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F_1 score of 75.82% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public.","tags":["NER","Arabic"],"title":"Arabic Named Entity Recognition: What Works and What's Next","type":"publication"},{"authors":["Liyuan Liu","Zihan Wang","Jingbo Shang","Dandong Yin","Heng Ji","Xiang Ren","Shaowen Wang","Jiawei Han"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"339259da4eb6c8dd7f7ad56f0d337cfe","permalink":"https://liyuanlucasliu.github.io/publication/liu-raw-end-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/liu-raw-end-2019/","section":"publication","summary":"Taking word sequences as the input, typical named entity recognition (NER) models neglect errors from pre-processing (e.g., tokenization). However, these errors can influence the model performance greatly, especially for noisy texts like tweets. Here, we introduce Neural-Char-CRF, a raw-to-end framework that is more robust to pre-processing errors. It takes raw character sequences as inputs and makes end-to-end predictions. Word embedding and contextualized representation models are further tailored to capture textual signals for each character instead of each word. Our model neither requires the conversion from character sequences to word sequences, nor assumes tokenizer can correctly detect all word boundaries. Moreover, we observe our model performance remains unchanged after replacing tokenization with string matching, which demonstrates its potential to be tokenization-free. Extensive experimental results on two public datasets demonstrate the superiority of our proposed method over the state of the art. The implementations and datasets are made available at: https://github.com/LiyuanLucasLiu/Raw-to-End.","tags":["Social Media","NER"],"title":"Raw-to-End Name Entity Recognition in Social Media","type":"publication"},{"authors":["Jingbo Shang","Jiaming Shen","Liyuan Liu","Jiawei Han"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"5ce89b606a8a4f23f385914d605955ec","permalink":"https://liyuanlucasliu.github.io/publication/shang-constructing-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/shang-constructing-2019/","section":"publication","summary":"Real-world data exists largely in the form of unstructured texts. A grand challenge on data mining research is to develop effective and scalable methods that may transform unstructured text into structured knowledge. Based on our vision, it is highly beneficial to transform such text into structured heterogeneous information networks, on which actionable knowledge can be generated based on the user's need. In this tutorial, we provide a comprehensive overview on recent research and development in this direction. First, we introduce a series of effective methods that construct heterogeneous information networks from massive, domain-specific text corpora. Then we discuss methods that mine such text-rich networks based on the user's need. Specifically, we focus on scalable, effective, weakly supervised, language-agnostic methods that work on various kinds of text. We further demonstrate, on real datasets (including news articles, scientific publications, and product reviews), how information networks can be constructed and how they can assist further exploratory analysis.","tags":["Entity Recognition","Network","Phrase","Taxonomy","Tutorial"],"title":"Constructing and Mining Heterogeneous Information Networks from Massive Text","type":"publication"},{"authors":["Ying Lin","Liyuan Liu","Heng Ji","Dong Yu","Jiawei Han"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"a4830e85171916b8097997127d591c2a","permalink":"https://liyuanlucasliu.github.io/publication/lin-reliability-aware-2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/lin-reliability-aware-2019/","section":"publication","summary":"Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5% absolute F-score gain.","tags":["NER","Embedding"],"title":"Reliability-aware Dynamic Feature Composition for Name Tagging","type":"publication"},{"authors":["Yujin Yuan","Liyuan Liu","Siliang Tang","Zhongfei Zhang","Yueting Zhuang","Shiliang Pu","Fei Wu","Xiang Ren"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"99827d4354495fe835a97fa69485ba79","permalink":"https://liyuanlucasliu.github.io/publication/yuan-cross-relation-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/yuan-cross-relation-2019/","section":"publication","summary":"Distant supervision leverages knowledge bases to automatically label instances, thus allowing us to train relation extractor without human annotations. However, the generated training data typically contain massive noise, and may result in poor performances with the vanilla supervised learning. In this paper, we propose to conduct multi-instance learning with a novel Cross-relation Cross-bag Selective Attention (C2SA), which leads to noise-robust training for distant supervised relation extractor. Specifically, we employ the sentence-level selective attention to reduce the effect of noisy or mismatched sentences, while the correlation among relations were captured to improve the quality of attention weights. Moreover, instead of treating all entity-pairs equally, we try to pay more attention to entity-pairs with a higher quality. Similarly, we adopt the selective attention mechanism to achieve this goal. Experiments with two types of relation extractor demonstrate the superiority of the proposed approach over the state-of-the-art, while further ablation studies verify our intuitions and demonstrate the effectiveness of our proposed two techniques.","tags":["Relation Extraction","Distant Supervision","Attention"],"title":"Cross-Relation Cross-Bag Attention for Distantly-Supervised Relation Extraction","type":"publication"},{"authors":["Liyuan Liu","Xiang Ren","Jingbo Shang","Jian Peng","Jiawei Han"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"93b61360aafb41c632197411ff54e136","permalink":"https://liyuanlucasliu.github.io/publication/liu-efficient-2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/liu-efficient-2018/","section":"publication","summary":"Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (PTLM), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large models, even in the inference stage, lead to overwhelming computation workloads, thus making them too time-consuming for real-world applications. For a specific task, we aim to keep useful information while compressing bulky PTLM. Since layers of different depths keep different information, we can conduct the compression via layer selection. By introducing the dense connectivity, we can detach any layers without eliminating others, and stretch shallow and wide LMs to be deep and narrow. Moreover, PTLM are trained with layer-wise dropouts for better robustness, and are pruned by a sparse regularization which is customized for our goal. Experiments on benchmarks demonstrate the effectiveness of our proposed method.","tags":["NER","Language Model","Sequence Labeling","Pruning","Selected"],"title":"Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling","type":"publication"},{"authors":["Jingbo Shang*","Liyuan Liu","Xiaotao Gu","Xiang Ren","Teng Ren","Jiawei Han"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"212b06d0f9d0d6ac4bc65590bfafdbce","permalink":"https://liyuanlucasliu.github.io/publication/shang-learning-2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/shang-learning-2018/","section":"publication","summary":"Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.","tags":["NER","Distant Supervision","Selected"],"title":"Learning Named Entity Tagger using Domain-Specific Dictionary","type":"publication"},{"authors":["Huan Gui","Qi Zhu","Liyuan Liu","Aston Zhang","Jiawei Han"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"66cc845edc6b8037819423b022013d07","permalink":"https://liyuanlucasliu.github.io/publication/gui-expert-2018/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/gui-expert-2018/","section":"publication","summary":"Expert finding is an important task in both industry and academia. It is challenging to rank candidates with appropriate expertise for various queries. In addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. We study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content analysis and authority ranking. Regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow semantic meanings. Compared with global embedding learning, locally-trained embedding learning projects the terms into a latent semantic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. Considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of expert finding, provides additional information. Specifically, different types of interactions among objects play different roles. We propose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. To demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. The experiment results show that the proposed framework outperforms existing methods for both general and specific queries.","tags":["Expert Finding","Embedding"],"title":"Expert Finding in Heterogeneous Bibliographic Networks with Locally-trained Embeddings","type":"publication"},{"authors":["Liyuan Liu","Jingbo Shang","Xiang Ren","Frank Fangzheng Xu","Huan Gui","Jian Peng","Jiawei Han"],"categories":null,"content":"","date":1517529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517529600,"objectID":"296723a45e576da01059d0148bcfcf59","permalink":"https://liyuanlucasliu.github.io/publication/liu-empower-2018/","publishdate":"2018-02-02T00:00:00Z","relpermalink":"/publication/liu-empower-2018/","section":"publication","summary":"Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F_1 score of 91.71+/-0.10 without using any extra annotations.","tags":["NER","Language Model","Character-level","Sequence Labeling","Selected"],"title":"Empower Sequence Labeling with Task-Aware Neural Language Model","type":"publication"},{"authors":["Jingbo Shang","Xiyao Shi","Meng Jiang","Liyuan Liu","Timothy Hanratty","Jiawei Han"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"b4ad920568f79b85483992c762ca00c6","permalink":"https://liyuanlucasliu.github.io/publication/shang-contrast-2018/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/shang-contrast-2018/","section":"publication","summary":"Graph pattern mining methods can extract informative and useful patterns from large-scale graphs and capture underlying principles through the overwhelmed information. Contrast analysis serves as a keystone in various fields and has demonstrated its effectiveness in mining valuable information. However, it has been long overlooked in graph pattern mining. Therefore, in this paper, we introduce the concept of contrast subgraph, that is, a subset of nodes that have significantly different edges or edge weights in two given graphs of the same node set. The major challenge comes from the gap between the contrast and the informativeness. Because of the widely existing noise edges in real-world graphs, the contrast may lead to subgraphs of pure noise. To avoid such meaningless subgraphs, we leverage the similarity as the cornerstone of the contrast. Specifically, we first identify a coherent core, which is a small subset of nodes with similar edge structures in the two graphs, and then induce contrast subgraphs from the coherent cores. Moreover, we design a general family of coherence and contrast metrics and derive a polynomial-time algorithm to efficiently extract contrast subgraphs. Extensive experiments verify the necessity of introducing coherent cores as well as the effectiveness and efficiency of our algorithm. Real-world applications demonstrate the tremendous potentials of contrast subgraph mining.","tags":["Contrast","Network"],"title":"Contrast Subgraph Mining from Coherent Cores","type":"publication"},{"authors":["Carl Yang","Liyuan Liu","Mengxiong Liu","Zongyi Wang","Chao Zhang","Jiawei Han"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"afcd48c0418ebf54e9d48ab2a2d013a4","permalink":"https://liyuanlucasliu.github.io/publication/yang-graph-2017/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/yang-graph-2017/","section":"publication","summary":"Graph clustering (or community detection) has long drawn enormous attention from the research on web mining and information networks. Recent literature on this topic has reached a consensus that node contents and link structures should be integrated for reliable graph clustering, especially in an unsupervised setting. However, existing methods based on shallow models often suffer from content noise and sparsity. In this work, we propose to utilize deep embedding for graph clustering, motivated by the well-recognized power of neural networks in learning intrinsic content representations. Upon that, we capture the dynamic nature of networks through the principle of influence propagation and calculate the dynamic network embedding. Network clusters are then detected based on the stable state of such an embedding. Unlike most existing embedding methods that are task-agnostic, we simultaneously solve for the underlying node representations and the optimal clustering assignments in an end-to-end manner. To provide more insight, we theoretically analyze our interpretation of network clusters and find its underlying connections with two widely applied approaches for network modeling. Extensive experimental results on six real-world datasets including both social networks and citation networks demonstrate the superiority of our proposed model over the state-of-the-art.","tags":["Clustering","Network with Attributes","Propagation","Network"],"title":"Graph Clustering with Embedding Propagation","type":"publication"},{"authors":["Qi Zhu","Hongwei Ng","Liyuan Liu","Ziwei Ji","Bingjie Jiang","Jiaming Shen","Huan Gui"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"d1ff3abbe39bf05cfedd7913cd7d980c","permalink":"https://liyuanlucasliu.github.io/publication/zhu-wikidata-2017/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/zhu-wikidata-2017/","section":"publication","summary":"Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation. As it can be edited by anyone, entries frequently get vandalized, leading to the possibility that it might spread of falsified information if such posts are not detected. The WSDM 2017 Wiki Vandalism Detection Challenge requires us to solve this problem by computing a vandalism score denoting the likelihood that a revision corresponds to an act of vandalism and performance is measured using the ROC-AUC obtained on a held-out test set. This paper provides the details of our submission that obtained an ROC-AUC score of 0.91976 in the final evaluation.","tags":["Vandalism Detection","WSDM Cup"],"title":"Wikidata Vandalism Detection - The Loganberry Vandalism Detector at WSDM Cup 2017","type":"publication"},{"authors":["Liyuan Liu","Xiang Ren*","Qi Zhu","Shi Zhi","Huan Gui","Heng Ji","Jiawei Han"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"5498dbb311a27384e32550c517a52187","permalink":"https://liyuanlucasliu.github.io/publication/liu-heterogeneous-2017/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/liu-heterogeneous-2017/","section":"publication","summary":"Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.","tags":["Heterogeneous Supervision","Relation Extraction","Distant Supervision","Selected"],"title":"Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach","type":"publication"},{"authors":["Chao Zhang","Liyuan Liu","Dongming Lei","Quan Yuan","Honglei Zhuang","Tim Hanratty","Jiawei Han"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"38e906261fcb3f73164e5211cf55188a","permalink":"https://liyuanlucasliu.github.io/publication/zhang-triovecevent-2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/zhang-triovecevent-2017/","section":"publication","summary":"Detecting local events (e.g., protest, disaster) at their onsets is an important task for a wide spectrum of applications, ranging from disaster control to crime monitoring and place recommendation. Recent years have witnessed growing interest in leveraging geo-tagged tweet streams for online local event detection. Nevertheless, the accuracies of existing methods still remain unsatisfactory for building reliable local event detection systems. We propose TRIOVECEVENT, a method that leverages multimodal embeddings to achieve accurate online local event detection. The effectiveness of TRIOVECEVENT is underpinned by its two-step detection scheme. First, it ensures a high coverage of the underlying local events by dividing the tweets in the query window into coherent geo-topic clusters. To generate quality geo-topic clusters, we capture short-text semantics by learning multimodal embeddings of the location, time, and text, and then perform online clustering with a novel Bayesian mixture model. Second, TRIOVECEVENT considers the geo-topic clusters as candidate events and extracts a set of features for classifying the candidates. Leveraging the multimodal embeddings as background knowledge, we introduce discriminative features that can well characterize local events, which enable pinpointing true local events from the candidate pool with a small amount of training data. We have used crowdsourcing to evaluate TRIOVECEVENT, and found that it improves the performance of the state-of-the-art method by a large margin.","tags":["Social Media","Clustering","Event Detection","Embedding"],"title":"TrioVecEvent: Embedding-based Online Local Event Detection in Geo-tagged Tweet Streams","type":"publication"},{"authors":["Liyuan Liu","Linli Xu","Zhen Wang","Enhong Chen"],"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"3c23229bc4275bb7490ec06310a9da46","permalink":"https://liyuanlucasliu.github.io/publication/liu-community-2015/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/liu-community-2015/","section":"publication","summary":"With the recent advances in information networks, the problem of identifying group structure or communities has received a significant amount of attention. Most of the existing principles of community detection or clustering mainly focus on either the topological structure of a network or the node attributes separately, while both of the two aspects provide valuable information to characterize the nature of communities. In this paper we combine the topological structure of a network as well as the content information of nodes in the task of detecting communities in information networks. Specifically, we treat a network as a dynamic system and consider its community structure as a consequence of interactions among nodes. To model the interactions we introduce the principle of content propagation and integrate the aspects of structure and content in a network naturally. We further describe the interactions among nodes in two different ways, including a linear model to approximate influence propagation, and modeling the interactions directly with random walk. Based on interaction modeling, the nature of communities is described by analyzing the stable status of the dynamic system. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed framework over the state of the art.","tags":["Clustering","Propagation"],"title":"Community Detection Based on Structure and Content: A Content Propagation Perspective","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://liyuanlucasliu.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Honors and Experience","tags":null,"title":"Honors and Experiences","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4fbb8782f5e7b376f7fdf0f2b4c34325","permalink":"https://liyuanlucasliu.github.io/honor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/honor/","section":"","summary":"Honors","tags":null,"title":"Honors and Experiences","type":"widget_page"}]