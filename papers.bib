
@inproceedings{zhang_triovecevent:_2017,
	title = {{TrioVecEvent}: {Embedding}-based online local event detection in geo-tagged tweet streams},
	volume = {Part F1296},
	copyright = {All rights reserved},
	doi = {10.1145/3097983.3098027},
	abstract = {Detecting local events (e.g., protest, disaster) at their onsets is an important task for a wide spectrum of applications, ranging from disaster control to crime monitoring and place recommendation. Recent years have witnessed growing interest in leveraging geo-tagged tweet streams for online local event detection. Nevertheless, the accuracies of existing methods still remain unsatisfactory for building reliable local event detection systems. We propose TRIOVECEVENT, a method that leverages multimodal embeddings to achieve accurate online local event detection. The effectiveness of TRIOVECEVENT is underpinned by its two-step detection scheme. First, it ensures a high coverage of the underlying local events by dividing the tweets in the query window into coherent geo-topic clusters. To generate quality geo-topic clusters, we capture short-text semantics by learning multimodal embeddings of the location, time, and text, and then perform online clustering with a novel Bayesian mixture model. Second, TRIOVECEVENT considers the geo-topic clusters as candidate events and extracts a set of features for classifying the candidates. Leveraging the multimodal embeddings as background knowledge, we introduce discriminative features that can well characterize local events, which enable pinpointing true local events from the candidate pool with a small amount of training data. We have used crowdsourcing to evaluate TRIOVECEVENT, and found that it improves the performance of the state-of-the-art method by a large margin.},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Zhang, Chao and Liu, Liyuan and Lei, Dongming and Yuan, Quan and Zhuang, Honglei and Hanratty, Tim and Han, Jiawei},
	month = aug,
	year = {2017},
	note = {ISBN: 9781450348874},
	pages = {595--604},
	file = {PDF:/Users/ll2/Zotero/storage/LH2HWHZ3/Zhang et al. - 2017 - TrioVecEvent Embedding-Based Online Local Event Detection in Geo-Tagged Tweet Streams.pdf:}
}

@inproceedings{liu_community_2015,
	title = {Community {Detection} {Based} on {Structure} and {Content}: {A} {Content} {Propagation} {Perspective}},
	copyright = {All rights reserved},
	doi = {10.1109/ICDM.2015.105},
	abstract = {â€”With the recent advances in information networks, the problem of identifying group structure or communities has received a significant amount of attention. Most of the existing principles of community detection or clustering mainly focus on either the topological structure of a network or the node attributes separately, while both of the two aspects provide valuable information to characterize the nature of communities. In this paper we combine the topological structure of a network as well as the content information of nodes in the task of detecting communities in information networks. Specifically, we treat a network as a dynamic system and consider its community structure as a consequence of interactions among nodes. To model the interactions we introduce the principle of content propagation and integrate the aspects of structure and content in a network naturally. We further describe the interactions among nodes in two different ways, including a linear model to approximate influence propagation, and modeling the interactions directly with random walk. Based on interaction modeling, the nature of communities is described by analyzing the stable status of the dynamic system. Extensive experimental results on benchmark datasets demonstrate the superiority of the proposed framework over the state of the art.},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Liu, Liyuan and Xu, Linli and Wang, Zhen and Chen, Enhong},
	month = nov,
	year = {2015},
	keywords = {community detection, content propagation},
	file = {PDF:/Users/ll2/Zotero/storage/PB8GCCJ6/Liu et al. - Unknown - Community Detection Based on Structure and Content A Content Propagation Perspective(2).pdf:application/pdf}
}

@inproceedings{shang_learning_2018,
	title = {Learning {Named} {Entity} {Tagger} using {Domain}-{Specific} {Dictionary}},
	copyright = {All rights reserved},
	url = {https://github.com/shangjingbo1226/},
	doi = {10.18653/v1/d18-1230},
	abstract = {Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.},
	urldate = {2018-09-03},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Shang, Jingbo and Liu, Liyuan and Gu, Xiaotao and Ren, Xiang and Ren, Teng and Han, Jiawei},
	month = oct,
	year = {2018},
	note = {arXiv: 1809.03599},
	pages = {2054--2064},
	file = {PDF:/Users/ll2/Zotero/storage/DVV9ZGLG/Shang et al. - Unknown - Learning Named Entity Tagger using Domain-Specific Dictionary.pdf:application/pdf}
}

@inproceedings{liu_efficient_2018,
	title = {Efficient {Contextualized} {Representation}: {Language} {Model} {Pruning} for {Sequence} {Labeling}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1804.07827},
	abstract = {Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (PTLM), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large models, even in the inference stage, lead to overwhelming computation workloads, thus making them too time-consuming for real-world applications. For a specific task, we aim to keep useful information while compressing bulky PTLM. Since layers of different depths keep different information, we can conduct the compression via layer selection. By introducing the dense connectivity, we can detach any layers without eliminating others, and stretch shallow and wide LMs to be deep and narrow. Moreover, PTLM are trained with layer-wise dropouts for better robustness, and are pruned by a sparse regularization which is customized for our goal. Experiments on benchmarks demonstrate the effectiveness of our proposed method.},
	urldate = {2018-08-06},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Liu, Liyuan and Ren, Xiang and Shang, Jingbo and Peng, Jian and Han, Jiawei},
	month = oct,
	year = {2018},
	note = {arXiv: 1804.07827},
	file = {PDF:/Users/ll2/Zotero/storage/DJ8VH7NQ/Liu et al. - 2018 - Efficient Contextualized Representation Language Model Pruning for Sequence Labeling.pdf:application/pdf}
}

@inproceedings{Liu2019,
	title = {On the {Variance} of the {Adaptive} {Learning} {Rate} and {Beyond}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1908.03265},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
	urldate = {2019-08-14},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Learning} {Representations}},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	month = apr,
	year = {2020},
	note = {arXiv: 1908.03265},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/Users/ll2/Zotero/storage/3NXUZPRF/Liu et al. - 2019 - On the Variance of the Adaptive Learning Rate and Beyond.pdf:application/pdf;arXiv Fulltext PDF:/Users/ll2/Zotero/storage/F37T9VAR/Liu et al. - 2019 - On the Variance of the Adaptive Learning Rate and .pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/8M5M4GJR/1908.html:text/html}
}

@inproceedings{liu_heterogeneous_2017,
	title = {Heterogeneous {Supervision} for {Relation} {Extraction}: {A} {Representation} {Learning} {Approach}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1707.00166},
	abstract = {Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Liu, Liyuan and Ren, Xiang and Zhu, Qi and Zhi, Shi and Gui, Huan and Ji, Heng and Han, Jiawei},
	year = {2017},
	note = {arXiv: 1707.00166},
	file = {PDF:/Users/ll2/Zotero/storage/CBAZURQ5/Liu et al. - 2017 - Heterogeneous Supervision for Relation Extraction A Representation Learning Approach.pdf:application/pdf;Full Text:/Users/ll2/Zotero/storage/ECSIRB8D/Liu et al. - 2017 - Heterogeneous supervision for relation extraction.pdf:application/pdf;Snapshot:/Users/ll2/Zotero/storage/DIPWF6Y5/1707.html:text/html}
}

@inproceedings{liu_empower_2018,
	title = {Empower {Sequence} {Labeling} with {Task}-{Aware} {Neural} {Language} {Model}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneysâ€™ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the authorâ€™s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the authorâ€™s employer, and then only on the authorâ€™s or the employerâ€™s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the authorâ€™s or the employerâ€™s creation (including tables of contents with links to other papers) without AAAIâ€™s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17123},
	abstract = {Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F\_1 score of 91.71+/-0.10 without using any extra annotations.},
	language = {en},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Liyuan and Shang, Jingbo and Ren, Xiang and Xu, Frank Fangzheng and Gui, Huan and Peng, Jian and Han, Jiawei},
	month = feb,
	year = {2018},
	keywords = {NER},
	file = {Full Text PDF:/Users/ll2/Zotero/storage/PLCPH63Y/Liu et al. - 2018 - Empower Sequence Labeling with Task-Aware Neural L.pdf:application/pdf;Snapshot:/Users/ll2/Zotero/storage/IJTB73CQ/17123.html:text/html}
}

@inproceedings{wang_crossweigh_2019,
	title = {{CrossWeigh}: {Training} {Named} {Entity} {Tagger} from {Imperfect} {Annotations}},
	copyright = {All rights reserved},
	shorttitle = {{CrossWeigh}},
	url = {http://arxiv.org/abs/1909.01441},
	abstract = {Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38\% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93\%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh.},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	author = {Wang, Zihan and Shang, Jingbo and Liu, Liyuan and Lu, Lihao and Liu, Jiacheng and Han, Jiawei},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.01441},
	keywords = {Computer Science - Computation and Language, NER},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/J2QCKZS6/Wang et al. - 2019 - CrossWeigh Training Named Entity Tagger from Impe.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/SVNWT4HV/1909.html:text/html}
}

@inproceedings{ye_looking_2019,
	address = {Hong Kong, China},
	title = {Looking {Beyond} {Label} {Noise}: {Shifted} {Label} {Distribution} {Matters} in {Distantly} {Supervised} {Relation} {Extraction}},
	copyright = {All rights reserved},
	shorttitle = {Looking {Beyond} {Label} {Noise}},
	url = {https://www.aclweb.org/anthology/D19-1397},
	doi = {10.18653/v1/D19-1397},
	abstract = {In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23\% relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ye, Qinyuan and Liu, Liyuan and Zhang, Maosen and Ren, Xiang},
	month = nov,
	year = {2019},
	keywords = {shifted label distribution},
	pages = {3841--3850},
	file = {Full Text PDF:/Users/ll2/Zotero/storage/XZGKK3DW/Ye et al. - 2019 - Looking Beyond Label Noise Shifted Label Distribu.pdf:application/pdf;Full Text PDF:/Users/ll2/Zotero/storage/IWCVC78I/Ye et al. - 2019 - Looking Beyond Label Noise Shifted Label Distribu.pdf:application/pdf}
}

@inproceedings{liu_raw--end_2019,
	title = {Raw-to-{End} {Name} {Entity} {Recognition} in {Social} {Media}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1908.05344},
	abstract = {Taking word sequences as the input, typical named entity recognition (NER) models neglect errors from pre-processing (e.g., tokenization). However, these errors can influence the model performance greatly, especially for noisy texts like tweets. Here, we introduce Neural-Char-CRF, a raw-to-end framework that is more robust to pre-processing errors. It takes raw character sequences as inputs and makes end-to-end predictions. Word embedding and contextualized representation models are further tailored to capture textual signals for each character instead of each word. Our model neither requires the conversion from character sequences to word sequences, nor assumes tokenizer can correctly detect all word boundaries. Moreover, we observe our model performance remains unchanged after replacing tokenization with string matching, which demonstrates its potential to be tokenization-free. Extensive experimental results on two public datasets demonstrate the superiority of our proposed method over the state of the art. The implementations and datasets are made available at: https://github.com/LiyuanLucasLiu/Raw-to-End.},
	urldate = {2020-02-23},
	booktitle = {{arXiv}:1908.05344 [cs]},
	author = {Liu, Liyuan and Wang, Zihan and Shang, Jingbo and Yin, Dandong and Ji, Heng and Ren, Xiang and Wang, Shaowen and Han, Jiawei},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05344},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, NER},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/US2A8MCU/Liu et al. - 2019 - Raw-to-End Name Entity Recognition in Social Media.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/XWUG4DZP/1908.html:text/html;arXiv Fulltext PDF:/Users/ll2/Zotero/storage/CVGYYAYP/Liu et al. - 2019 - Raw-to-End Name Entity Recognition in Social Media.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/9PDSDHZH/1908.html:text/html}
}

@inproceedings{yuan_cross-relation_2019,
	title = {Cross-{Relation} {Cross}-{Bag} {Attention} for {Distantly}-{Supervised} {Relation} {Extraction}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/3813},
	doi = {10.1609/aaai.v33i01.3301419},
	abstract = {Distant supervision leverages knowledge bases to automatically label instances, thus allowing us to train relation extractor without human annotations. However, the generated training data typically contain massive noise, and may result in poor performances with the vanilla supervised learning. In this paper, we propose to conduct multi-instance learning with a novel Cross-relation Cross-bag Selective Attention (C2SA), which leads to noise-robust training for distant supervised relation extractor. Specifically, we employ the sentence-level selective attention to reduce the effect of noisy or mismatched sentences, while the correlation among relations were captured to improve the quality of attention weights. Moreover, instead of treating all entity-pairs equally, we try to pay more attention to entity-pairs with a higher quality. Similarly, we adopt the selective attention mechanism to achieve this goal. Experiments with two types of relation extractor demonstrate the superiority of the proposed approach over the state-of-the-art, while further ablation studies verify our intuitions and demonstrate the effectiveness of our proposed two techniques.},
	language = {en},
	urldate = {2020-02-28},
	booktitle = {Thirty-{Third} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Yuan, Yujin and Liu, Liyuan and Tang, Siliang and Zhang, Zhongfei and Zhuang, Yueting and Pu, Shiliang and Wu, Fei and Ren, Xiang},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {419--426},
	file = {Full Text PDF:/Users/ll2/Zotero/storage/K3EBXIX4/Yuan et al. - 2019 - Cross-Relation Cross-Bag Attention for Distantly-S.pdf:application/pdf;Snapshot:/Users/ll2/Zotero/storage/FNT5K28K/3813.html:text/html}
}

@inproceedings{liu_arabic_2019,
	address = {Florence, Italy},
	title = {Arabic {Named} {Entity} {Recognition}: {What} {Works} and {What}'s {Next}},
	copyright = {All rights reserved},
	shorttitle = {Arabic {Named} {Entity} {Recognition}},
	url = {https://www.aclweb.org/anthology/W19-4607},
	doi = {10.18653/v1/W19-4607},
	abstract = {This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F\_1 score of 75.82\% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public.},
	urldate = {2020-02-28},
	booktitle = {Proceedings of the {Fourth} {Arabic} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Liyuan and Shang, Jingbo and Han, Jiawei},
	month = aug,
	year = {2019},
	pages = {60--67},
	file = {Full Text PDF:/Users/ll2/Zotero/storage/5ZU57K35/Liu et al. - 2019 - Arabic Named Entity Recognition What Works and Wh.pdf:application/pdf}
}

@inproceedings{lin_reliability-aware_2019,
	address = {Florence, Italy},
	title = {Reliability-aware {Dynamic} {Feature} {Composition} for {Name} {Tagging}},
	copyright = {All rights reserved},
	url = {https://www.aclweb.org/anthology/P19-1016},
	doi = {10.18653/v1/P19-1016},
	abstract = {Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7\% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5\% absolute F-score gain.},
	urldate = {2020-02-28},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Ying and Liu, Liyuan and Ji, Heng and Yu, Dong and Han, Jiawei},
	month = jul,
	year = {2019},
	pages = {165--174},
	file = {Full Text PDF:/Users/ll2/Zotero/storage/5862R45J/Lin et al. - 2019 - Reliability-aware Dynamic Feature Composition for .pdf:application/pdf}
}

@inproceedings{shang_constructing_2019,
	address = {Anchorage, AK, USA},
	series = {{KDD} '19},
	title = {Constructing and {Mining} {Heterogeneous} {Information} {Networks} from {Massive} {Text}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3332275},
	doi = {10.1145/3292500.3332275},
	abstract = {Real-world data exists largely in the form of unstructured texts. A grand challenge on data mining research is to develop effective and scalable methods that may transform unstructured text into structured knowledge. Based on our vision, it is highly beneficial to transform such text into structured heterogeneous information networks, on which actionable knowledge can be generated based on the user's need. In this tutorial, we provide a comprehensive overview on recent research and development in this direction. First, we introduce a series of effective methods that construct heterogeneous information networks from massive, domain-specific text corpora. Then we discuss methods that mine such text-rich networks based on the user's need. Specifically, we focus on scalable, effective, weakly supervised, language-agnostic methods that work on various kinds of text. We further demonstrate, on real datasets (including news articles, scientific publications, and product reviews), how information networks can be constructed and how they can assist further exploratory analysis.},
	urldate = {2020-02-28},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Shang, Jingbo and Shen, Jiaming and Liu, Liyuan and Han, Jiawei},
	month = jul,
	year = {2019},
	keywords = {entity recognition, massive text corpora, network mining and applications, phrase mining, taxonomy construction},
	pages = {3191--3192}
}

@inproceedings{yang_graph_2017,
	title = {Graph {Clustering} with {Dynamic} {Embedding}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1712.08249},
	abstract = {Graph clustering (or community detection) has long drawn enormous attention from the research on web mining and information networks. Recent literature on this topic has reached a consensus that node contents and link structures should be integrated for reliable graph clustering, especially in an unsupervised setting. However, existing methods based on shallow models often suffer from content noise and sparsity. In this work, we propose to utilize deep embedding for graph clustering, motivated by the well-recognized power of neural networks in learning intrinsic content representations. Upon that, we capture the dynamic nature of networks through the principle of influence propagation and calculate the dynamic network embedding. Network clusters are then detected based on the stable state of such an embedding. Unlike most existing embedding methods that are task-agnostic, we simultaneously solve for the underlying node representations and the optimal clustering assignments in an end-to-end manner. To provide more insight, we theoretically analyze our interpretation of network clusters and find its underlying connections with two widely applied approaches for network modeling. Extensive experimental results on six real-world datasets including both social networks and citation networks demonstrate the superiority of our proposed model over the state-of-the-art.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1712.08249 [physics]},
	author = {Yang, Carl and Liu, Mengxiong and Wang, Zongyi and Liu, Liyuan and Han, Jiawei},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.08249},
	keywords = {Computer Science - Social and Information Networks, Physics - Physics and Society},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/YK984W9M/Yang et al. - 2017 - Graph Clustering with Dynamic Embedding.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/RCV9IX4R/1712.html:text/html}
}

@inproceedings{zhu_wikidata_2017,
	title = {Wikidata {Vandalism} {Detection} - {The} {Loganberry} {Vandalism} {Detector} at {WSDM} {Cup} 2017},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1712.06922},
	abstract = {Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation. As it can be edited by anyone, entries frequently get vandalized, leading to the possibility that it might spread of falsified information if such posts are not detected. The WSDM 2017 Wiki Vandalism Detection Challenge requires us to solve this problem by computing a vandalism score denoting the likelihood that a revision corresponds to an act of vandalism and performance is measured using the ROC-AUC obtained on a held-out test set. This paper provides the details of our submission that obtained an ROC-AUC score of 0.91976 in the final evaluation.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1712.06922 [cs]},
	author = {Zhu, Qi and Ng, Hongwei and Liu, Liyuan and Ji, Ziwei and Jiang, Bingjie and Shen, Jiaming and Gui, Huan},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06922},
	keywords = {Computer Science - Information Retrieval, H.3},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/YSUAWVU8/Zhu et al. - 2017 - Wikidata Vandalism Detection - The Loganberry Vand.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/N66D3JF2/1712.html:text/html}
}

@inproceedings{gui_expert_2018,
	title = {Expert {Finding} in {Heterogeneous} {Bibliographic} {Networks} with {Locally}-trained {Embeddings}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1803.03370},
	abstract = {Expert finding is an important task in both industry and academia. It is challenging to rank candidates with appropriate expertise for various queries. In addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. We study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content analysis and authority ranking. Regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow semantic meanings. Compared with global embedding learning, locally-trained embedding learning projects the terms into a latent semantic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. Considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of expert finding, provides additional information. Specifically, different types of interactions among objects play different roles. We propose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. To demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. The experiment results show that the proposed framework outperforms existing methods for both general and specific queries.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1803.03370 [cs]},
	author = {Gui, Huan and Zhu, Qi and Liu, Liyuan and Zhang, Aston and Han, Jiawei},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03370},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/JGS2FTB7/Gui et al. - 2018 - Expert Finding in Heterogeneous Bibliographic Netw.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/4F34298I/1803.html:text/html}
}

@inproceedings{mao_facet-aware_2019,
	title = {Facet-{Aware} {Evaluation} for {Extractive} {Text} {Summarization}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1908.10383},
	abstract = {Commonly adopted metrics for extractive text summarization like ROUGE focus on the lexical similarity and are facet-agnostic. In this paper, we present a facet-aware evaluation procedure for better assessment of the information coverage in extracted summaries while still supporting automatic evaluation once annotated. Specifically, we treat {\textbackslash}textit\{facet\} instead of {\textbackslash}textit\{token\} as the basic unit for evaluation, manually annotate the {\textbackslash}textit\{support sentences\} for each facet, and directly evaluate extractive methods by comparing the indices of extracted sentences with support sentences. We demonstrate the benefits of the proposed setup by performing a thorough {\textbackslash}textit\{quantitative\} investigation on the CNN/Daily Mail dataset, which in the meantime reveals useful insights of state-of-the-art summarization methods.{\textbackslash}footnote\{\vphantom{\}}Data can be found at {\textbackslash}url\{https://github.com/morningmoni/FAR\}.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1908.10383 [cs]},
	author = {Mao, Yuning and Liu, Liyuan and Zhu, Qi and Ren, Xiang and Han, Jiawei},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.10383},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/WKEKTZD6/Mao et al. - 2019 - Facet-Aware Evaluation for Extractive Text Summari.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/PK4C7N9D/1908.html:text/html}
}

@inproceedings{shang_contrast_2018,
	title = {Contrast {Subgraph} {Mining} from {Coherent} {Cores}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1802.06189},
	abstract = {Graph pattern mining methods can extract informative and useful patterns from large-scale graphs and capture underlying principles through the overwhelmed information. Contrast analysis serves as a keystone in various fields and has demonstrated its effectiveness in mining valuable information. However, it has been long overlooked in graph pattern mining. Therefore, in this paper, we introduce the concept of contrast subgraph, that is, a subset of nodes that have significantly different edges or edge weights in two given graphs of the same node set. The major challenge comes from the gap between the contrast and the informativeness. Because of the widely existing noise edges in real-world graphs, the contrast may lead to subgraphs of pure noise. To avoid such meaningless subgraphs, we leverage the similarity as the cornerstone of the contrast. Specifically, we first identify a coherent core, which is a small subset of nodes with similar edge structures in the two graphs, and then induce contrast subgraphs from the coherent cores. Moreover, we design a general family of coherence and contrast metrics and derive a polynomial-time algorithm to efficiently extract contrast subgraphs. Extensive experiments verify the necessity of introducing coherent cores as well as the effectiveness and efficiency of our algorithm. Real-world applications demonstrate the tremendous potentials of contrast subgraph mining.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1802.06189 [cs]},
	author = {Shang, Jingbo and Shi, Xiyao and Jiang, Meng and Liu, Liyuan and Hanratty, Timothy and Han, Jiawei},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06189},
	keywords = {Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/ATDJ5M8Y/Shang et al. - 2018 - Contrast Subgraph Mining from Coherent Cores.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/YLZ69SCJ/1802.html:text/html}
}

@inproceedings{lan_learning_2019,
	title = {Learning to {Contextually} {Aggregate} {Multi}-{Source} {Supervision} for {Sequence} {Labeling}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1910.04289},
	abstract = {Sequence labeling is a fundamental framework for various natural language processing problems. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios. In many cases, ground truth labels are costly and time-consuming to collect or even non-existent, while imperfect ones could be easily accessed or transferred from different domains. In this paper, we propose a novel framework named consensus Network (ConNet) to conduct training with imperfect annotations from multiple sources. It learns the representation for every weak supervision source and dynamically aggregates them by a context-aware attention mechanism. Finally, it leads to a model reflecting the consensus among multiple sources. We evaluate the proposed framework in two practical settings of multisource learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1910.04289 [cs]},
	author = {Lan, Ouyu and Huang, Xiao and Lin, Bill Yuchen and Jiang, He and Liu, Liyuan and Ren, Xiang},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.04289},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ll2/Zotero/storage/IRSTKKRE/Lan et al. - 2019 - Learning to Contextually Aggregate Multi-Source Su.pdf:application/pdf;arXiv.org Snapshot:/Users/ll2/Zotero/storage/C7RCF97V/1910.html:text/html}
}