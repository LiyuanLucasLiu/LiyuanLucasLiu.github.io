<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog -- coding is the real-world magic | Liyuan Liu</title>
    <link>https://liyuanlucasliu.github.io/blog/</link>
      <atom:link href="https://liyuanlucasliu.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog -- coding is the real-world magic</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright © Liyuan Liu</copyright><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://liyuanlucasliu.github.io/img/avatar.jpg</url>
      <title>Blog -- coding is the real-world magic</title>
      <link>https://liyuanlucasliu.github.io/blog/</link>
    </image>
    
    <item>
      <title>How to Covert T-Mobile One Plus 7T to the International Version</title>
      <link>https://liyuanlucasliu.github.io/blog/2021-01-oneplus/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2021-01-oneplus/</guid>
      <description>&lt;p&gt;I shattered my phone screen several days ago, and decided to move my stuff from ios to Android, due to the huge price difference. Eventually, I managed to get an unlocked T-Mobile OnePlus 7T from B&amp;amp;H at about 300$. The phone is unlocked, but the system is the T-Mobile version, meaning all updates would be released from T-Mobile after a delay and the start screen will have a huge T-Mobile logo.&lt;/p&gt;
&lt;p&gt;To get rid all of these, I flashed my system into the international version system, following 
&lt;a href=&#34;https://forum.xda-developers.com/t/t-mobile-7t-conversion-to-international-without-unlocked-bootloader-sim-unlock.4153943/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;. Yet, as this is my first time dealing with Android, I met some problems, and the whole process toke me about three whole hours, motivating me to add some notes to the original guide.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Download the patched MSM download tool: &lt;a href=&#34;https://drive.google.com/file/d/12y3lO0wIgb2_LSDeVw9x_2C_HjbgZlb_/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/12y3lO0wIgb2_LSDeVw9x_2C_HjbgZlb_/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An easy first step.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Second step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Run the tool, connect your phone in EDL mode (see below how to do it if you don&amp;rsquo;t know), ensure the &amp;ldquo;sha256 check&amp;rdquo; is NOT checked, click the Start button and wait until it&amp;rsquo;s completed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The second is not that easy. To get through, you need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install the Qualcomm HS-USB QDLoader 9008 Driver (device manager -&amp;gt; find the phone in other devices -&amp;gt; update driver -&amp;gt; install the optional windows update)&lt;/li&gt;
&lt;li&gt;To enter the EDL mode:
&lt;blockquote&gt;
&lt;p&gt;power off the phone and disconnect the USB if it&amp;rsquo;s connected. Then press both volume UP and DOWN at same time, then while holding those buttons down, insert the USB which should already be connected to your computer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;If you decide to quit, you can download a 
&lt;a href=&#34;https://forum.xda-developers.com/t/op7t-oos-10-0-15-hd65aa-ba-unbrick-tool-to-restore-your-device-to-oxygenos.3994835/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;different MSM tool&lt;/a&gt; to revert the system back to the T-Mobile system&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Third Step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The phone will show a red warning saying &amp;ldquo;device is corrupt&amp;rdquo;, just ignore that and wait some time. It will fallback in fastboot because it&amp;rsquo;s not able to boot. Update your fastboot executable, otherwise you&amp;rsquo;ll encounter errors and modem issues. Download it here: &lt;a href=&#34;https://dl.google.com/android/repository/platform-tools-latest-windows.zip&#34;&gt;https://dl.google.com/android/repository/platform-tools-latest-windows.zip&lt;/a&gt;. Now run the following fastboot command &lt;code&gt;fastboot flashing unlock_critical&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I changed the order of this part a little bit. It is necessary to download the 
&lt;a href=&#34;https://dl.google.com/android/repository/usb_driver_r13-windows.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Android Usb Driver&lt;/a&gt;, and install the driver via the step 39+ in 
&lt;a href=&#34;https://www.teamandroid.com/2012/07/30/how-to-set-up-adb-fastboot-with-android-sdk/3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt;. In the end, a selection UI will be available (volume up/down, and power for operation).&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Fourth Step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Bootloader is now unlocked.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally!&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Fifth Step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Extract the contents of the 
&lt;a href=&#34;https://forum.xda-developers.com/t/rom-stock-fastboot-op7t-stock-fastboot-roms-for-oneplus-7t.3979213/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Global Fastboot Rom zip&lt;/a&gt; folder directly into your ADB folder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you did this correctly your ADB folder will be filled with a bunch of new .img files. Like boot.img​
Open CMD (with admin rights) and navigate to your ADB folder. run &lt;code&gt;flash-all.bat&lt;/code&gt; (on PC).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;I recommend to install an Not-Up-To-Date version here, and then update later. In my case, I need to repeat this process twice. After the first time, T-Mobile logo will still show up, and the wifi is not available. I entered the system, go to setting, first go to &lt;code&gt;About 7T&lt;/code&gt; and tap &amp;hellip; three times to enable developer mode, then go to &amp;hellip; and turn on advanced boot choice and reboot into fastboot again. Re-flash the system with &lt;code&gt;flash-all.bat&lt;/code&gt; again. now it would have the choice to select language after roboot, but DO NOTHING HERE!&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Final Step&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;You&amp;rsquo;ve now fully converted to the Global Firmware. Confirm this by going to settings &amp;gt; about phone and look at Build Number. 
Update the system to the latest firmwarm. 
Lock the bootloader again by first entering the fastboot, then check to make sure phone is being picked up by bootloader via &lt;code&gt;fastboot devices&lt;/code&gt; (on PC), and finally relock bootloader via &lt;code&gt;fastboot oem lock&lt;/code&gt; (on PC).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Welcome to the international version OnePlus 7T!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One Sentence Summary for EMNLP 2020</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-11-emnlp/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-11-emnlp/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding
&lt;ul&gt;
&lt;li&gt;Position encoding learns local position information in the encoder and absolute position in the decoder.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A Matter of Framing: The Impact of Linguistic Formalism on Probing Results
&lt;ul&gt;
&lt;li&gt;Choice of linguistic formalism is important for role-semantic probing results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contrastive Distillation on Intermediate Representations for Language Model Compression
&lt;ul&gt;
&lt;li&gt;Language model compression via contrastive loss, while memory bank is used to store negative samples (not updated in training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Efficient Meta Lifelong-Learning with Limited Memory
&lt;ul&gt;
&lt;li&gt;Efficient experience rehearsal is achieved by learn to select examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks
&lt;ul&gt;
&lt;li&gt;Introduced optimization-based meta-learning to language modeling pretraining-finetuning paradigm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference
&lt;ul&gt;
&lt;li&gt;Multi-head attention can be viewed as multiple sample from the same distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Token-level Adaptive Training for Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Using frequency-based heuristic to re-weight the loss value for different tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shallow-to-Deep Training for Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Both copying-strategy and lr-reset matters for progressive Transformer-based NMT training (copying is more important and &lt;code&gt;123-&amp;gt;123123&lt;/code&gt; &amp;gt; &lt;code&gt;123-&amp;gt;112233&lt;/code&gt; &amp;gt; &lt;code&gt;123-&amp;gt;123333&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Incorporating a Local Translation Mechanism into Non-autoregressive Translation
&lt;ul&gt;
&lt;li&gt;In NAT, treating an autoregressive sequence instead of a token as the basic unit (for &lt;code&gt;1234&lt;/code&gt; generate &lt;code&gt;12&lt;/code&gt; and &lt;code&gt;34&lt;/code&gt; simultaneously while &lt;code&gt;2&lt;/code&gt; is generated after &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;To alleviate error accumulation, use attention mask to change classical attention (global, all tokens are accessible) to local attention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masking as an Efficient Alternative to Finetuning for Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;Instead of finetuning LMs, learn a weight mask instead (e.g., &lt;code&gt;[0.1, 0.3] -&amp;gt; [0.0, 0.3]&lt;/code&gt;), which is argued to be more memory-efficient (1-bit instead of 32/16-bit additional copy).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If Beam Search is the Answer, What was the Question?
&lt;ul&gt;
&lt;li&gt;Beam search has an inductive bias which can be linked to the promotion of uniform information
density &amp;mdash; variance of surprisals is linear to BLEU (people dont want surprise in reading).&lt;/li&gt;
&lt;li&gt;By adding regularization on information density, exact search performs similar to beam search.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When BERT Plays the Lottery, All Tickets Are Winning
&lt;ul&gt;
&lt;li&gt;The pruned “good” subnetworks works well, while the “bad” ones do not.&lt;/li&gt;
&lt;li&gt;For structured pruning, even “bad” subnetworks can be finetuned separately to reach fairly strong performance.&lt;/li&gt;
&lt;li&gt;Different runs get different “good” subnetworks, indicating the existance of factors otherthan non-trivial linguistic features.&lt;/li&gt;
&lt;li&gt;The success of BERT might be more related to optimization surfaces rather than specific bits
of linguistic knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning
&lt;ul&gt;
&lt;li&gt;Using RL to select context for NMT at the document-level.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Pruning in training is much better than pruning after converge (0.1 BLEU drop v.s. 1.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT
&lt;ul&gt;
&lt;li&gt;Pre-training on high-resource -&amp;gt; fine-tune on low-resource by expanding dictionary -&amp;gt; XLM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems
&lt;ul&gt;
&lt;li&gt;Char-level NMT &amp;lt; first train Subword-level NMT, then convert to Character-level &amp;lt; Subword-level NMT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Jointly learn to select training data from comparable (rather than parallel) data, and to NMT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can Automatic Post-Editing Improve NMT?
&lt;ul&gt;
&lt;li&gt;With more training data, automatic post-editing can improve NMT a lot.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning from Context or Names? An Empirical Study on Neural Relation Extraction
&lt;ul&gt;
&lt;li&gt;Both context and names are important for performance, while existing RE datasets may leak cues via names.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some Languages Seem Easier to Parse Because Their Treebanks Leak
&lt;ul&gt;
&lt;li&gt;Some languages seem easier to parse, since the trees in the test set are mostly isomorphic with some tree in the training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Word Frequency Does Not Predict Grammatical Knowledge in Language
Models
&lt;ul&gt;
&lt;li&gt;Comparing &lt;code&gt;The cat walks&lt;/code&gt; and &lt;code&gt;The cat walk&lt;/code&gt;, the author found in four dimensions, frequency is not related to how well the knowledge is learned.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do sequence-to-sequence VAEs learn global features of sentences?
&lt;ul&gt;
&lt;li&gt;VAEs are prone to memorizing the first words and the sentence length, producing local
features of limited usefulness, while chaning architecture can help to alleviate such memorization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial Semantic Collisions
&lt;ul&gt;
&lt;li&gt;Semantic collisions refer to texts that are semantically unrelated but judged as similar by NLP models, which can be effectively derived with gradient based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sparse Text Generation
&lt;ul&gt;
&lt;li&gt;Entmax transformation is leveraged to train and sample from a natively sparse language model, while \alpha-Entmax unifies softmax, argmax, and sparsemax.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning VariationalWord Masks to Improve the Interpretability of Neural Text Classifiers
&lt;ul&gt;
&lt;li&gt;word masks are automatically learned to emphasize task-specific words, which improves interpretability and performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Identifying Elements Essential for BERT’s Multilinguality
&lt;ul&gt;
&lt;li&gt;shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality.&lt;/li&gt;
&lt;li&gt;Word order is relevant: BERT is not multilingual with one language having an inverted word order.&lt;/li&gt;
&lt;li&gt;The comparability of training corpora contributes to multilinguality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A Streaming Approach For Efficient Batched Beam Search
&lt;ul&gt;
&lt;li&gt;Periodically “refills” the batch before proceeding with a selected subset of candidates, special handlings are made to handle the self-attention padding.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;On Losses for Modern Language Models
&lt;ul&gt;
&lt;li&gt;Clarify NSP’s effect on BERT pre-training and design more auxiliary tasks, which allows the final method outperforms BERTBase with fewer than a quarter of the training tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entities as Experts: Sparse Memory Access with Entity Supervision
&lt;ul&gt;
&lt;li&gt;Leveraging entity representations to better memorize sparse knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic Label Smoothing for Sequence to Sequence Problems
&lt;ul&gt;
&lt;li&gt;Retrieve semantically similar sentences to do label smoothing, at the cost of training computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We Can Detect Your Bias: Predicting the Political Ideology of News Articles
&lt;ul&gt;
&lt;li&gt;Directly using BERT would result in a heavy dependency on source, which motivates the leverage of adversarial training to neutralize such leaked clues.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Imitation Attacks and Defenses for Black-box Machine Translation Systems
&lt;ul&gt;
&lt;li&gt;After imitating black-box machine translation system, attack the imitated system with universal trigger attack / suffix dropper attack / targeted flip attack.&lt;/li&gt;
&lt;li&gt;Discussions are included to defense model stealing with gradient poisoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three
&lt;ul&gt;
&lt;li&gt;Ensemble helps to calibration and improve model performance, while distillation helps to make it faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Inference Strategies for Machine Translation with Conditional Masking
&lt;ul&gt;
&lt;li&gt;Using masked language model as decoder, and iterative replacing masks with model outputs.&lt;/li&gt;
&lt;li&gt;Four strategies are used: fixed #iterations; fixed #tokens/iteration; probs &amp;gt; T; comined-probs &amp;gt; T (best)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An Empirical Study of Generation Order for Machine Translation
&lt;ul&gt;
&lt;li&gt;Using Transformer to do generation in an insertive manner, i.e., &lt;code&gt;(a, c)-&amp;gt;b&lt;/code&gt;, many efforts are made on exploring different generation orders (balanced binary tree works the best, while others are not far behind)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems
&lt;ul&gt;
&lt;li&gt;Benchmark for hyper-parameter optimization on NMT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation
&lt;ul&gt;
&lt;li&gt;Learn to generate masking, for adapting language model to a new domain in a faster manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting
&lt;ul&gt;
&lt;li&gt;For small dataset, adding additional regularization on the parameter wight shift (difference to the original pre-trained weights)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning
&lt;ul&gt;
&lt;li&gt;After pre-training NER models on a source domain, trying to adapt the model to a new domain by conducting structured nearest neighbor search (similarities are calculated based on representations constructed by the pre-trained model).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks
&lt;ul&gt;
&lt;li&gt;After linearization labeled sentence (converting into &lt;code&gt;B-PER Jose E-PER&lt;/code&gt;), language modeling is trained and used to generate new data for the training.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models
&lt;ul&gt;
&lt;li&gt;Pre-training LSTMs on music helps to train it on language later, but not random generated texts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
&lt;ul&gt;
&lt;li&gt;LM does not perform well on numerical commonsense knowledge, even with distant supervision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Train No Evil: Selective Masking for Task-Guided Pre-Training
&lt;ul&gt;
&lt;li&gt;Finding important tokens with heuristic strategies and conducting task-guided pre-training (heuristic leverages task-specific information).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fact or Fiction: Verifying Scientific Claims
&lt;ul&gt;
&lt;li&gt;Propose a new task called scientific claim verification and construct corresponding datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language Model Prior for Low-Resource Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Using language model outputs to regularize NMT in a knowledge distillation manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My Work-From-Home Secrets</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-08-wfh/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-08-wfh/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Chairs:
&lt;ul&gt;
&lt;li&gt;I purchased a 
&lt;a href=&#34;https://www.ebay.com/itm/Steelcase-Leap-Chair-V2-Open-Box-Fully-Loaded-Black-Fabric/121882349228&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;refurbished leap v2 chair&lt;/a&gt; from madison seating. Its website has some horrible reviews, but its ebay shop seems legit. Still, for refurbished chairs, they are used and YMMV.&lt;/li&gt;
&lt;li&gt;My girlfriend is using a chair from 
&lt;a href=&#34;https://www.autonomous.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Autonomous&lt;/a&gt;, it is also great. This compant sells some fully functioning seats in a relative low price (although I cannot use chrome to open their homepage on Windows&amp;hellip;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Desk:
&lt;ul&gt;
&lt;li&gt;Big NO to rising desks (or adjustable height desks). Although I love these desks, the dealbreaker is that, sooner or later, its motors are gonna break, which is super troublesome.&lt;/li&gt;
&lt;li&gt;Easy and reliable alternatives are the 
&lt;a href=&#34;https://www.amazon.com/s?k=furniture&amp;#43;risers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;furniture rise&lt;/a&gt; + 
&lt;a href=&#34;https://www.ikea.com/us/en/p/linnmon-adils-table-white-s29932181/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cheapest IKEA table&lt;/a&gt; combination.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Monitor stand:
&lt;ul&gt;
&lt;li&gt;Monitor stands are awesome! Save lots of desk space and greatly simplifies the workspace.&lt;/li&gt;
&lt;li&gt;Note that if you are using 32&amp;rsquo;&amp;rsquo; monitor/heavy monitor, be sure to buy monitor stand with a heavy load (only the ones with gas spring arms can hold my monitor).&lt;/li&gt;
&lt;li&gt;I also attach 
&lt;a href=&#34;https://www.amazon.com/TotalMount-Monitor-Mount-for-Headphones/dp/B083JLDPDP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;headphone mounts&lt;/a&gt; to my monitor. They look great!&lt;/li&gt;
&lt;li&gt;I also mount my laptop on my monitor stand with an additional spring arm&amp;ndash; 
&lt;a href=&#34;https://www.amazon.com/s?k=vesa&amp;#43;laptop&amp;#43;mount&amp;#43;tray&amp;amp;ref=nb_sb_noss&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VESA laptop mount tray works great&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mouse and Keyboards:
&lt;ul&gt;
&lt;li&gt;Logitech MX Master is the only mouse I would recommend (v1, v2, v3 are all great).&lt;/li&gt;
&lt;li&gt;Any of the following keyboards are great:
&lt;ul&gt;
&lt;li&gt;Microsoft Ergonomic Keyboard (cheapest, wired and slow actuation)&lt;/li&gt;
&lt;li&gt;Microsoft Sculpt Ergonomic Keyboard (wireless, not bluetooth enabled, using batteries and fast actuation)&lt;/li&gt;
&lt;li&gt;Logitech MX Keys (not ergonomic, wireless, bluetooth enabled, rechargeable and fast actuation)&lt;/li&gt;
&lt;li&gt;Logitech K860 (wireless, bluetooth enabled, using batteries and fast actuation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Configuration Cheatsheet</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-07-setup/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-07-setup/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Docker
&lt;ul&gt;
&lt;li&gt;Nvidia Image: 
&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/running.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorRT&lt;/a&gt;, 
&lt;a href=&#34;https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/running.html#running&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker pull nvcr.io/nvidia/tensorrt:20.03-py3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull nvcr.io/nvidia/pytorch:20.03-py3&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Commands:
&lt;ul&gt;
&lt;li&gt;remove image: &lt;code&gt;docker rmi $name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;prune: &lt;code&gt;docker system prune&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Permission Error:
&lt;ul&gt;
&lt;li&gt;fixing by &lt;code&gt;sudo chmod 666 /var/run/docker.sock&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No enough space:
&lt;ul&gt;
&lt;li&gt;moving to a different disk by
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo cp -r /var/lib/docker $TARGET_PATH
echo &#39;{
&amp;quot;graph&amp;quot;: &amp;quot;$TARGET_PATH&amp;quot;
}&#39; | sudo tee -a /etc/docker/daemon.json
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo chmod 666 /var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jupyter
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/ipython-contrib/jupyter_contrib_nbextensions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Extensions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;conda install -c conda-forge jupyter_contrib_nbextensions&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Windows
&lt;ul&gt;
&lt;li&gt;Terminal: Termius (education account w. github student)&lt;/li&gt;
&lt;li&gt;Windows Subsystem for Linux:
&lt;ul&gt;
&lt;li&gt;install:
&lt;ul&gt;
&lt;li&gt;run: &lt;code&gt;Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform&lt;/code&gt; in powershell w. admin&lt;/li&gt;
&lt;li&gt;run: &lt;code&gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux&lt;/code&gt; in powershell w. admin&lt;/li&gt;
&lt;li&gt;get Ubuntu from microsoft store &amp;amp; RUN the app (&lt;code&gt;Ubuntu&lt;/code&gt; refers to the recommended version, should be 20.04)&lt;/li&gt;
&lt;li&gt;run: &lt;code&gt;wsl --set-default-version 2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Now, the ubuntu&amp;rsquo;s &lt;code&gt;bash.exe&lt;/code&gt; is available:
&lt;ul&gt;
&lt;li&gt;windows file system is mounted at &lt;code&gt;/mnt/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;anyconnect not working in WSL:
&lt;ul&gt;
&lt;li&gt;use anyconnect within the windows store. 
&lt;a href=&#34;https://github.com/microsoft/WSL/issues/4277&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Windows Terminal:
&lt;ul&gt;
&lt;li&gt;Configuration is saved at 
&lt;a href=&#34;windows_terminal_settings.json&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-Monitor:
&lt;ul&gt;
&lt;li&gt;Enable multi task bar (search &lt;code&gt;taskbar&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Make the larger one major monitor and neven group icons (search &lt;code&gt;display&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Only show the icon of searching (right click at taskbar-&amp;gt;search)&lt;/li&gt;
&lt;li&gt;Hide &lt;code&gt;cortana&lt;/code&gt; and &lt;code&gt;task&lt;/code&gt; buttons by right click at taskbar.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Win-PE creator:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.wepe.com.cn/&#34;&gt;http://www.wepe.com.cn/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sublime:
&lt;ul&gt;
&lt;li&gt;install package control w &lt;code&gt;install package control&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;theme: default (or ayu)&lt;/li&gt;
&lt;li&gt;sftp: for windows: convert &lt;code&gt;ssh&lt;/code&gt; to &lt;code&gt;ppk&lt;/code&gt; with puttygen in &lt;a href=&#34;https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html&#34;&gt;https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;setting: &lt;code&gt;&amp;quot;ssh_key_file&amp;quot;: &amp;quot;C:/Users/Liyuan Liu - LL/.ssh/id_rsa.ppk&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remote editing alternative: &lt;code&gt;https://github.com/textmate/rmate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;sidebarenhancement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tmux
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/tmux-plugins/tmux-resurrect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;oh my tmux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tmux 1.8 config:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;https://github.com/mandre/dotfiles/blob/master/.tmux-1.8.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;save and restore tmux section:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/tmux-plugins/tpm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tmux-plugin-manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/tmux-plugins/tmux-resurrect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tmux-resurrent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;save pane content by commands
&lt;ul&gt;
&lt;li&gt;command to use in tmux: &lt;code&gt;tmux capture-pane -pS -1000000 &amp;gt; file.out&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;bash commands
&lt;ul&gt;
&lt;li&gt;generate
&lt;ul&gt;
&lt;li&gt;basic: &lt;code&gt;ssh-keygen&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;advanced: &lt;code&gt;ssh-keygen -t idrsa -b 4096 -C &amp;quot;llychinalz@gmail.com&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;authorize server login w. ssh
&lt;ul&gt;
&lt;li&gt;recommended:&lt;code&gt;ssh-copy-id -i idrsa $remote_username@$server_ip_address -p $port&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;add ssh key:
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;eval `ssh-agent`
ssh-add idrsa
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linux
&lt;ul&gt;
&lt;li&gt;check linux version: &lt;code&gt;uname -m &amp;amp;&amp;amp; cat /etc/*release&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;check cuda version:&lt;code&gt;cat /usr/local/cuda/version.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;output gpu utils to one line:&lt;code&gt;nvidia-smi --query-gpu=memory.used --format=csv,noheader | xargs | sed -e &#39;s/ /,/g&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;reboot to bios: &lt;code&gt;sudo systemctl reboot --firmware-setup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;autojump install
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install autojump
echo &amp;quot;. /usr/share/autojump/autojump.sh&amp;quot; | tee -a ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;zsh
&lt;ul&gt;
&lt;li&gt;oh-my-zsh &lt;code&gt;sh -c &amp;quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;zsh-syntax-highlighting &lt;code&gt;git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;zsh-autosuggestions &lt;code&gt;git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;plug-in permissions &lt;code&gt;compaudit | xargs chmod g-w,o-w&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;on WSL, the git plut-in may be too slow, turn off by &lt;code&gt;git config --global oh-my-zsh.hide-status 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;driver
&lt;ul&gt;
&lt;li&gt;To install the Display Driver, the Nouveau drivers must first be disabled.
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-nouveau&#34;&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-nouveau&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NAS
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rrshare&lt;/code&gt; with updated API
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/koolob/rrshare&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker image and readme&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nvidia Geforce GPU
&lt;ul&gt;
&lt;li&gt;restrict base clock and temperature to protect cards:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/164907210&#34;&gt;https://zhuanlan.zhihu.com/p/164907210&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PyTorch
&lt;ul&gt;
&lt;li&gt;solution for &lt;code&gt;Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL...&lt;/code&gt;:
&lt;ul&gt;
&lt;li&gt;python: &lt;code&gt;os.environ[&#39;MKL_THREADING_LAYER&#39;] = &#39;GNU&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;command: &lt;code&gt;MKL_THREADING_LAYER=GNU python my_script.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;env: &lt;code&gt;export MKL_THREADING_LAYER=GNU&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;distributed data parallel halts without error message:
&lt;ul&gt;
&lt;li&gt;env: &lt;code&gt;export NCCL_P2P_DISABLE=1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RStudio
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker run -d --name rstudio -v $HOME:/home/rstudio -e PASSWORD=$PASSWORD -p 8787:8787 rocker/tidyverse&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Markdown editor
&lt;ul&gt;
&lt;li&gt;Joplin + Typora&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Printer
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/yaurora/cups-google-airprint&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker repo source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;stop cups service for DSM
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;synoservice --hard-disable cupsd
synoservice --hard-disable cups-lpd
synoservicectl --stop cupsd
synoservicectl --stop cups-lpd
synoservicecfg --hard-disable cupsd
synoservicecfg --hard-disable cups-lpd
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;command to start:
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --name=&amp;quot;airprint&amp;quot; --net=&amp;quot;host&amp;quot; --privileged=&amp;quot;true&amp;quot; \
    -e &amp;quot;CUPS_USER_ADMIN&amp;quot;=$USER -e &amp;quot;CUPS_USER_PASSWORD&amp;quot;=$PASS \
    -v $PATH_TO_AIRPRINT_CONFIG:/config \
    -v /dev/bus/usb:/dev/bus/usb \
    yaurora/cups-google-airprint
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Login and install Canon DFRII driver:
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it airprint /bin/bash
wget http://gdlp01.c-wss.com/gds/6/0100009236/06/linux-UFRII-drv-v510-usen-09.tar.gz
tar -xzf linux-UFRII-drv-v510-usen-09.tar.gz
cd linux-UFRII-drv-v510-usen
./install.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Network address for windows:
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http://$IP/printers/Canon_D530_D560
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FP16 and Apex</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-03-fp16/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-03-fp16/</guid>
      <description>&lt;p&gt;For recent Nvidia GPUs like V100 or R8000, allowing half-precision training can get roughly 2X speedup instantly, and the only problem is &lt;strong&gt;how to conduct mixed-precision training?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unlike TPU, typical Nvidia GPUs follow the IEEE fp16 standard instead of bfp16.
Specifically, TPU uses the bfp16 standard with 8 exponent bits and 7 mantissa bits, while fp16 has only 5 exponent bits and 10 mantissa bits.
With less exponent bits, directly applying half-precision to deep learning leads to tons of overflow and underflow.&lt;/p&gt;
&lt;p&gt;To compensate the loss of exponent bits, dynamic loss scaling and mixed-precision training have been proposed.
The 
&lt;a href=&#34;https://github.com/pytorch/fairseq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fairseq&lt;/a&gt; package comes with naive fp16 support, and for custom models and other PyTorch codebases, the 
&lt;a href=&#34;https://github.com/NVIDIA/apex/tree/master/apex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apex&lt;/a&gt; package is usually the go-to choice.
Besides 
&lt;a href=&#34;https://nvidia.github.io/apex/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the apex documentation&lt;/a&gt;, below provides some tips for mixed-precision training.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-loss-scaling&#34;&gt;Dynamic Loss Scaling&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;TL;DL.
Since it is easier to detect overflow then under, a minimal loss scale is recommended to set (e.g., 0.03125) and a small window helps to stabilize the training (e.g., 256).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unlike underflow, the overflow can be easily detected.
Accordingly, it is strategically benefical to make the model overflow to avoid underflow.
Specifically, &lt;strong&gt;loss scaling&lt;/strong&gt; is proposed to first scale-up the loss and gradients by a constant, then divide the gradient by the same constant after back-propagation.
Still, the choice of the constant is an open problem.
&lt;strong&gt;Dynamic loss scaling&lt;/strong&gt; is leveraged to dynamically update this constant during training.
Specifically, it starts from setting the constant to a large value, then halve its value when an overflow occurs, which indicates the constant value is probably too large.
Also, if no overflow is detected in N continuous updates which indicates the constant could be larger, it duplicates the constant.&lt;/p&gt;
&lt;h2 id=&#34;mixed-precision-training&#34;&gt;Mixed Precision Training&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;TL;DL.
Elementwise_affine parameters are easier to overflow and it is helpful to cast this part to fp32.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In apex, &lt;code&gt;opt_level&lt;/code&gt; can be set to &lt;code&gt;O0&lt;/code&gt; (full fp32), &lt;code&gt;O1&lt;/code&gt; (mixed precision), &lt;code&gt;O2&lt;/code&gt; (almost fp16), and &lt;code&gt;O3&lt;/code&gt; (full fp16).&lt;/li&gt;
&lt;li&gt;To specifically cast a model to fp32:
&lt;ul&gt;
&lt;li&gt;set model parameters, e.g.,
&lt;pre&gt;&lt;code&gt;for n, p in model.named_parameters():
    if any([ki in n for ki in fp32_keys]):
        p.float()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;cast precision conversion by monkey patching, e.g.,
&lt;pre&gt;&lt;code&gt;orig_linear = torch.nn.functional.linear
def wrapped_linear(*args):
 casted_args = []
  for arg in args:
    if torch.is_tensor(arg) and torch.is_floating_point(arg):
      casted_args.append(arg.float())
    else:
      casted_args.append(arg)
  return orig_linear(*casted_args)
torch.nn.functional.linear = wrapped_linear
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NextCloud Docker Setup</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-03-nextcloud/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-03-nextcloud/</guid>
      <description>&lt;p&gt;I found NextCloud to be very powerful, flexible and expandable.
Specifically, I setup NextCloud on my synology NAS via docker, manually upload my file (while will be essentially faster) for the initial sync, then use NextCloud as a combination of Dropbox and Google Drive.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Setup a new network for the docker;&lt;/li&gt;
&lt;li&gt;Setup &lt;code&gt;path&lt;/code&gt; as a folder to store data;&lt;/li&gt;
&lt;li&gt;Using the following docker compose file to build docker images (sensitive information is masked in &lt;code&gt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&lt;/code&gt;). The command for set up is &lt;code&gt;docker-compose up -d&lt;/code&gt;, and the command for update is &lt;code&gt;sudo docker-compose pull &amp;amp;&amp;amp; sudo docker-compose up -d&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Login with setting database to &lt;code&gt;mariadb&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;After manually uploading files to the destination, you can add files to nextcloud by &lt;code&gt;sudo -u www-data php occ files:scan --path &amp;quot;&amp;lt;username&amp;gt;/files/&amp;lt;path_under_files&amp;gt;&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;To recursively delete, use &lt;code&gt;find . -type d -name &#39;@eaDir&#39; -exec rm -r {} +&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Remember to add trusted domains.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-docker-compose.yml&#34;&gt;version: &#39;3.2&#39;
services:
  nextcloud:
    image: nextcloud
    container_name: nextcloud
    restart: always
    networks:
      - net
    ports:
      - 3222:80
    labels:
      - &amp;quot;traefik.docker.network=net&amp;quot;
      - &amp;quot;traefik.enable=true&amp;quot;
      - &amp;quot;traefik.backend=nextcloud&amp;quot;
      - &amp;quot;traefik.port=80&amp;quot;
      - &amp;quot;traefik.protocol=http&amp;quot;
      - &amp;quot;traefik.frontend.rule=Host:&amp;lt;domain name&amp;gt;&amp;quot;
      - &amp;quot;traefik.frontend.headers.customResponseHeaders=Strict-Transport-Security:15552000&amp;quot;
      - &amp;quot;traefik.frontend.passHostHeader=true&amp;quot;
      - &amp;quot;traefik.frontend.redirect.permanent:true&amp;quot;
      - &amp;quot;traefik.frontend.redirect.regex:https://(.*)/.well-known/(card|cal)dav&amp;quot;
      - &amp;quot;traefik.frontend.redirect.replacement:https://$$1/remote.php/dav/&amp;quot;
    depends_on:
      - nextcloud_db
    volumes:
      - &amp;lt;path&amp;gt;/nextcloud:/var/www/html
      - &amp;lt;path&amp;gt;/config:/var/www/html/config
      - &amp;lt;path&amp;gt;/data:/var/www/html/data
      - &amp;quot;MYSQL_HOST=nextcloud_db&amp;quot;
      - &amp;quot;NEXTCLOUD_TRUSTED_DOMAINS=&amp;lt;domain&amp;gt;&amp;quot;
      - &amp;quot;REDIS_HOST=nextcloud_redis&amp;quot;
    depends_on:
      - nextcloud_db
      - redis

  nextcloud_db:
    image: mariadb
    container_name: nextcloud_db
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    restart: always
    networks:
      - net
    volumes:
      - &amp;lt;path&amp;gt;/db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD=&amp;lt;password&amp;gt;
      - MYSQL_PASSWORD=&amp;lt;password&amp;gt;
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud

  redis:
    image: redis
    container_name: nextcloud_redis
    networks:
      - net

  cron:
    image: nextcloud
    container_name: nextcloud_cron
    restart: always
    networks:
      - net
    volumes:
      - &amp;lt;path&amp;gt;/nextcloud:/var/www/html
    entrypoint: /cron.sh
    depends_on:
      - nextcloud_db
      - redis

volumes:
  nextcloud:
  nextcloud_db:
  nextcloud_cron:

networks:
  net:
    external: true
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How to Save You (Make You Fast), Time Machine</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-02-tm/</link>
      <pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-02-tm/</guid>
      <description>&lt;p&gt;Last night, my Macbook Pro 2015 broke from an unsuccessful program install and could not start up. &lt;strong&gt;Time machine saves me&lt;/strong&gt;, &lt;strong&gt;again&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Data is priceless and duplication is the only way to protect it.
Time Machine is a useful program, however, its speed could be deadly slow if you are using a remote storage (instead of directly connected via usb or other cable).&lt;/p&gt;
&lt;p&gt;After many trial and error attempts, my current solution is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use 
&lt;a href=&#34;https://nextcloud.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NextCloud&lt;/a&gt; to sync codes, images, and other non-application non-system files. If you are also a Synology user, this platform is much better than Drive 2.0;&lt;/li&gt;
&lt;li&gt;use Time Machine to backup only system files and applications, with the speed-up setting as below (use the &lt;strong&gt;SMB&lt;/strong&gt; protocol instead of the &lt;strong&gt;AFP&lt;/strong&gt; protocol for Synology).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even only a small portion of my files (~100 Gb) is handled by Time Machine, they could be a very slow 100 Gb without speeding up.
The reason is the system sets an upper bound on the performance of Time Machine by default.
Running the following code would save you tons of time:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sysctl debug.lowpri_throttle_enabled=0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change it back after the first &amp;amp; major backups with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sysctl debug.lowpri_throttle_enabled=1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Zotero, Importing from Mendeley</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-02-zotero/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-02-zotero/</guid>
      <description>&lt;p&gt;After reaching the 2 Gb free space limit of Mendeley, I did some research and decide to use Zotero as my pdf management tool.
Different from Mendeley, Zotero supports WebDAV-based sync.&lt;/p&gt;
&lt;p&gt;However, importing pdfs from Mendelay from Zotero is not easy, since the latest Mendelay release encrypts its database.
Below is some notes on how to hack the importing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;sync Mendelay to the cloud and download all pdfs.&lt;/li&gt;
&lt;li&gt;delete the Mendelay library and app, move downloaded pdfs to a different folder;&lt;/li&gt;
&lt;li&gt;download an old version Mendelay, sync from the cloud, and add pdfs from the previous folder;&lt;/li&gt;
&lt;li&gt;export to zotero&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Hands Down, Best Router within 100$: K3C (which costs 30 USD)</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-02-k3c/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-02-k3c/</guid>
      <description>&lt;p&gt;Since I need a router to turn the printer (gift from 
&lt;a href=&#34;https://shangjingbo1226.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jingbo&lt;/a&gt;) into a internet printer, I did some research on cheap-but-powerful routers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;K3C costs about &lt;strong&gt;30$&lt;/strong&gt; but performs like a &lt;strong&gt;200$&lt;/strong&gt; router.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Upgrading to the un-official firearm is highly recommended, which unlocks tons of features, including Ads-blocking, USB-printer, etc.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://blog.iytc.net/wordpress/?p=2841#comments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A nice tutorial is available in Chinese&lt;/a&gt;.
However, no available tutorial is for K3C-international (i.e., can be brought from U.S.).
After some tiny-little explorations, I successfully finished the upgrade.
Here are some notes based on my experience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;upgrading to unofficial versions is risky; all operations are at your own risk;&lt;/li&gt;
&lt;li&gt;the version number of international-K3C firearm is in a different serial (from the tutorial), so ignore all version information in the tutorial;&lt;/li&gt;
&lt;li&gt;update the official firearm to the latest version, and then do the upgrade (I did this and it worked for me);&lt;/li&gt;
&lt;li&gt;after upgrade to unofficial 1.1, please go ahead and upgrade to unofficial 1.6 (1.7 is not very stable and not recommended);&lt;/li&gt;
&lt;li&gt;you need a windows pc for the upgrade&amp;hellip; (based on the tutorial);&lt;/li&gt;
&lt;li&gt;some links in the tutorial page is outdated, please find the files as below:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Necessary files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://liyuanlucasliu.github.io/files/blog-k3c/telnet.zip&#34; target=&#34;_blank&#34;&gt;Telnet Tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://liyuanlucasliu.github.io/files/blog-k3c/k3c_v11.tar.gz&#34; target=&#34;_blank&#34;&gt;V1.1 Firearm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://liyuanlucasliu.github.io/files/blog-k3c/k3c_v16.zip&#34; target=&#34;_blank&#34;&gt;V1.6 Firearm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
