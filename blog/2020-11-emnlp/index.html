<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Liyuan Liu">

  
  
  
    
  
  <meta name="description" content="One sentence summary for each paper I read during EMNLP 2020.">

  
  <link rel="alternate" hreflang="en-us" href="https://liyuanlucasliu.github.io/blog/2020-11-emnlp/">

  


  
  
  
  <meta name="theme-color" content="#E94A36">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huf8c3f0a9c54db469e58ad7146e34a4a2_11050_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huf8c3f0a9c54db469e58ad7146e34a4a2_11050_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://liyuanlucasliu.github.io/blog/2020-11-emnlp/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@LiyuanLucas">
  <meta property="twitter:creator" content="@LiyuanLucas">
  
  <meta property="og:site_name" content="Liyuan Liu">
  <meta property="og:url" content="https://liyuanlucasliu.github.io/blog/2020-11-emnlp/">
  <meta property="og:title" content="One Sentence Summary for EMNLP 2020 | Liyuan Liu">
  <meta property="og:description" content="One sentence summary for each paper I read during EMNLP 2020."><meta property="og:image" content="https://liyuanlucasliu.github.io/img/avatar.jpg">
  <meta property="twitter:image" content="https://liyuanlucasliu.github.io/img/avatar.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-11-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-11-01T00:00:00&#43;00:00">
  

  



  


  


  





  <title>One Sentence Summary for EMNLP 2020 | Liyuan Liu</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Liyuan Liu (Lucas)</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><p>Liyuan Liu</p></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#goal"><span>Goal</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Publications</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#publications"><span>Selected</span></a>
            
              <a class="dropdown-item" href="/publication/"><span>Full List</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Honors</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#honor"><span>Highlights</span></a>
            
              <a class="dropdown-item" href="/honor/"><span>All Honors</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Activities</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/#services"><span>Services</span></a>
            
              <a class="dropdown-item" href="/experience/"><span>Experience</span></a>
            
          </div>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
    </div>





      

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>One Sentence Summary for EMNLP 2020</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Nov 1, 2020
  </span>
  

  

  

  
  
  

  
  



  
  <div class="share-box" aria-hidden="true">
    <ul class="share">
      
        
        
        
          
        
        
        
        <li>
          <a href="https://twitter.com/intent/tweet?url=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/&amp;text=One%20Sentence%20Summary%20for%20EMNLP%202020" target="_blank" rel="noopener" class="share-btn-twitter">
            <i class="fab fa-twitter"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="https://www.facebook.com/sharer.php?u=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/&amp;t=One%20Sentence%20Summary%20for%20EMNLP%202020" target="_blank" rel="noopener" class="share-btn-facebook">
            <i class="fab fa-facebook"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="mailto:?subject=One%20Sentence%20Summary%20for%20EMNLP%202020&amp;body=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/" target="_blank" rel="noopener" class="share-btn-email">
            <i class="fas fa-envelope"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="https://www.linkedin.com/shareArticle?url=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/&amp;title=One%20Sentence%20Summary%20for%20EMNLP%202020" target="_blank" rel="noopener" class="share-btn-linkedin">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="https://reddit.com/submit?url=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/&amp;title=One%20Sentence%20Summary%20for%20EMNLP%202020" target="_blank" rel="noopener" class="share-btn-reddit">
            <i class="fab fa-reddit-alien"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="https://web.whatsapp.com/send?text=One%20Sentence%20Summary%20for%20EMNLP%202020%20https://liyuanlucasliu.github.io/blog/2020-11-emnlp/" target="_blank" rel="noopener" class="share-btn-whatsapp">
            <i class="fab fa-whatsapp"></i>
          </a>
        </li>
      
        
        
        
          
        
        
        
        <li>
          <a href="https://service.weibo.com/share/share.php?url=https://liyuanlucasliu.github.io/blog/2020-11-emnlp/&amp;title=One%20Sentence%20Summary%20for%20EMNLP%202020" target="_blank" rel="noopener" class="share-btn-weibo">
            <i class="fab fa-weibo"></i>
          </a>
        </li>
      
    </ul>
  </div>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <ol>
<li>What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding
<ul>
<li>Position encoding learns local position information in the encoder and absolute position in the decoder.</li>
</ul>
</li>
<li>A Matter of Framing: The Impact of Linguistic Formalism on Probing Results
<ul>
<li>Choice of linguistic formalism is important for role-semantic probing results.</li>
</ul>
</li>
<li>Contrastive Distillation on Intermediate Representations for Language Model Compression
<ul>
<li>Language model compression via contrastive loss, while memory bank is used to store negative samples (not updated in training).</li>
</ul>
</li>
<li>Efficient Meta Lifelong-Learning with Limited Memory
<ul>
<li>Efficient experience rehearsal is achieved by learn to select examples.</li>
</ul>
</li>
<li>Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks
<ul>
<li>Introduced optimization-based meta-learning to language modeling pretraining-finetuning paradigm.</li>
</ul>
</li>
<li>Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference
<ul>
<li>Multi-head attention can be viewed as multiple sample from the same distribution.</li>
</ul>
</li>
<li>Token-level Adaptive Training for Neural Machine Translation
<ul>
<li>Using frequency-based heuristic to re-weight the loss value for different tokens.</li>
</ul>
</li>
<li>Shallow-to-Deep Training for Neural Machine Translation
<ul>
<li>Both copying-strategy and lr-reset matters for progressive Transformer-based NMT training (copying is more important and <code>123-&gt;123123</code> &gt; <code>123-&gt;112233</code> &gt; <code>123-&gt;123333</code>)</li>
</ul>
</li>
<li>Incorporating a Local Translation Mechanism into Non-autoregressive Translation
<ul>
<li>In NAT, treating an autoregressive sequence instead of a token as the basic unit (for <code>1234</code> generate <code>12</code> and <code>34</code> simultaneously while <code>2</code> is generated after <code>1</code>)</li>
</ul>
</li>
<li>Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation
<ul>
<li>To alleviate error accumulation, use attention mask to change classical attention (global, all tokens are accessible) to local attention.</li>
</ul>
</li>
<li>Masking as an Efficient Alternative to Finetuning for Pretrained Language Models
<ul>
<li>Instead of finetuning LMs, learn a weight mask instead (e.g., <code>[0.1, 0.3] -&gt; [0.0, 0.3]</code>), which is argued to be more memory-efficient (1-bit instead of 32/16-bit additional copy).</li>
</ul>
</li>
<li>If Beam Search is the Answer, What was the Question?
<ul>
<li>Beam search has an inductive bias which can be linked to the promotion of uniform information
density &mdash; variance of surprisals is linear to BLEU (people dont want surprise in reading).</li>
<li>By adding regularization on information density, exact search performs similar to beam search.</li>
</ul>
</li>
<li>When BERT Plays the Lottery, All Tickets Are Winning
<ul>
<li>The pruned “good” subnetworks works well, while the “bad” ones do not.</li>
<li>For structured pruning, even “bad” subnetworks can be finetuned separately to reach fairly strong performance.</li>
<li>Different runs get different “good” subnetworks, indicating the existance of factors otherthan non-trivial linguistic features.</li>
<li>The success of BERT might be more related to optimization surfaces rather than specific bits
of linguistic knowledge.</li>
</ul>
</li>
<li>Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning
<ul>
<li>Using RL to select context for NMT at the document-level.</li>
</ul>
</li>
<li>Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation
<ul>
<li>Pruning in training is much better than pruning after converge (0.1 BLEU drop v.s. 1.1)</li>
</ul>
</li>
<li>Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT
<ul>
<li>Pre-training on high-resource -&gt; fine-tune on low-resource by expanding dictionary -&gt; XLM</li>
</ul>
</li>
<li>Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems
<ul>
<li>Char-level NMT &lt; first train Subword-level NMT, then convert to Character-level &lt; Subword-level NMT</li>
</ul>
</li>
<li>Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation
<ul>
<li>Jointly learn to select training data from comparable (rather than parallel) data, and to NMT.</li>
</ul>
</li>
<li>Can Automatic Post-Editing Improve NMT?
<ul>
<li>With more training data, automatic post-editing can improve NMT a lot.</li>
</ul>
</li>
<li>Learning from Context or Names? An Empirical Study on Neural Relation Extraction
<ul>
<li>Both context and names are important for performance, while existing RE datasets may leak cues via names.</li>
</ul>
</li>
<li>Some Languages Seem Easier to Parse Because Their Treebanks Leak
<ul>
<li>Some languages seem easier to parse, since the trees in the test set are mostly isomorphic with some tree in the training set.</li>
</ul>
</li>
<li>Word Frequency Does Not Predict Grammatical Knowledge in Language
Models
<ul>
<li>Comparing <code>The cat walks</code> and <code>The cat walk</code>, the author found in four dimensions, frequency is not related to how well the knowledge is learned.</li>
</ul>
</li>
<li>Do sequence-to-sequence VAEs learn global features of sentences?
<ul>
<li>VAEs are prone to memorizing the first words and the sentence length, producing local
features of limited usefulness, while chaning architecture can help to alleviate such memorization.</li>
</ul>
</li>
<li>Adversarial Semantic Collisions
<ul>
<li>Semantic collisions refer to texts that are semantically unrelated but judged as similar by NLP models, which can be effectively derived with gradient based methods.</li>
</ul>
</li>
<li>Sparse Text Generation
<ul>
<li>Entmax transformation is leveraged to train and sample from a natively sparse language model, while \alpha-Entmax unifies softmax, argmax, and sparsemax.</li>
</ul>
</li>
<li>Learning VariationalWord Masks to Improve the Interpretability of Neural Text Classifiers
<ul>
<li>word masks are automatically learned to emphasize task-specific words, which improves interpretability and performance.</li>
</ul>
</li>
<li>Identifying Elements Essential for BERT’s Multilinguality
<ul>
<li>shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality.</li>
<li>Word order is relevant: BERT is not multilingual with one language having an inverted word order.</li>
<li>The comparability of training corpora contributes to multilinguality.</li>
</ul>
</li>
<li>A Streaming Approach For Efficient Batched Beam Search
<ul>
<li>Periodically “refills” the batch before proceeding with a selected subset of candidates, special handlings are made to handle the self-attention padding.</li>
</ul>
</li>
<li>On Losses for Modern Language Models
<ul>
<li>Clarify NSP’s effect on BERT pre-training and design more auxiliary tasks, which allows the final method outperforms BERTBase with fewer than a quarter of the training tokens.</li>
</ul>
</li>
<li>Entities as Experts: Sparse Memory Access with Entity Supervision
<ul>
<li>Leveraging entity representations to better memorize sparse knowledge.</li>
</ul>
</li>
<li>Semantic Label Smoothing for Sequence to Sequence Problems
<ul>
<li>Retrieve semantically similar sentences to do label smoothing, at the cost of training computations.</li>
</ul>
</li>
<li>We Can Detect Your Bias: Predicting the Political Ideology of News Articles
<ul>
<li>Directly using BERT would result in a heavy dependency on source, which motivates the leverage of adversarial training to neutralize such leaked clues.</li>
</ul>
</li>
<li>Imitation Attacks and Defenses for Black-box Machine Translation Systems
<ul>
<li>After imitating black-box machine translation system, attack the imitated system with universal trigger attack / suffix dropper attack / targeted flip attack.</li>
<li>Discussions are included to defense model stealing with gradient poisoning.</li>
</ul>
</li>
<li>Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three
<ul>
<li>Ensemble helps to calibration and improve model performance, while distillation helps to make it faster.</li>
</ul>
</li>
<li>Inference Strategies for Machine Translation with Conditional Masking
<ul>
<li>Using masked language model as decoder, and iterative replacing masks with model outputs.</li>
<li>Four strategies are used: fixed #iterations; fixed #tokens/iteration; probs &gt; T; comined-probs &gt; T (best)</li>
</ul>
</li>
<li>An Empirical Study of Generation Order for Machine Translation
<ul>
<li>Using Transformer to do generation in an insertive manner, i.e., <code>(a, c)-&gt;b</code>, many efforts are made on exploring different generation orders (balanced binary tree works the best, while others are not far behind)</li>
</ul>
</li>
<li>Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems
<ul>
<li>Benchmark for hyper-parameter optimization on NMT.</li>
</ul>
</li>
<li>Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation
<ul>
<li>Learn to generate masking, for adapting language model to a new domain in a faster manner.</li>
</ul>
</li>
<li>Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting
<ul>
<li>For small dataset, adding additional regularization on the parameter wight shift (difference to the original pre-trained weights)</li>
</ul>
</li>
<li>Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning
<ul>
<li>After pre-training NER models on a source domain, trying to adapt the model to a new domain by conducting structured nearest neighbor search (similarities are calculated based on representations constructed by the pre-trained model).</li>
</ul>
</li>
<li>DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks
<ul>
<li>After linearization labeled sentence (converting into <code>B-PER Jose E-PER</code>), language modeling is trained and used to generate new data for the training.</li>
</ul>
</li>
<li>Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models
<ul>
<li>Pre-training LSTMs on music helps to train it on language later, but not random generated texts.</li>
</ul>
</li>
<li>Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
<ul>
<li>LM does not perform well on numerical commonsense knowledge, even with distant supervision.</li>
</ul>
</li>
<li>Train No Evil: Selective Masking for Task-Guided Pre-Training
<ul>
<li>Finding important tokens with heuristic strategies and conducting task-guided pre-training (heuristic leverages task-specific information).</li>
</ul>
</li>
<li>Fact or Fiction: Verifying Scientific Claims
<ul>
<li>Propose a new task called scientific claim verification and construct corresponding datasets.</li>
</ul>
</li>
<li>Language Model Prior for Low-Resource Neural Machine Translation
<ul>
<li>Using language model outputs to regularize NMT in a knowledge distillation manner.</li>
</ul>
</li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/emnlp/">EMNLP</a>
  
  <a class="badge badge-light" href="/tags/paper-summary/">Paper Summary</a>
  
</div>

















  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-square" src="/authors/liyuan-liu/avatar_hub04b2a45f3672cfc23ff6a0d82fcba88_258742_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://liyuanlucasliu.github.io/">Liyuan Liu</a></h5>
      <h6 class="card-subtitle">Ph.D. Candidate @ DMG</h6>
      <p class="card-text">Understand the underlying mechanism of neural NLP models &amp; Make the training more automatic, robust, and thus more resource-economical.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:ll2@illinois.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/LiyuanLucas" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/LiyuanLucasLiu" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=RmvbkzYAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-c"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>





  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.8e8b40c0fabefe5675ec2e4754a8f5cc.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <a href="https://www.hitwebcounter.com" target="_blank">
  <img src="https://hitwebcounter.com/counter/counter.php?page=6760300&style=0006&nbdigits=8&type=page&initCount=0" title="hit counts" Alt="hit counts" style="margin-left: auto;
  margin-right: auto">
  </a>

  <p class="powered-by">
    Copyright © Liyuan Liu &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
