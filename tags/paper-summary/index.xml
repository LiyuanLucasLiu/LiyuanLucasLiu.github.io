<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper Summary | Liyuan Liu</title>
    <link>https://liyuanlucasliu.github.io/tags/paper-summary/</link>
      <atom:link href="https://liyuanlucasliu.github.io/tags/paper-summary/index.xml" rel="self" type="application/rss+xml" />
    <description>Paper Summary</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright © Liyuan Liu</copyright><lastBuildDate>Sun, 01 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://liyuanlucasliu.github.io/img/avatar.jpg</url>
      <title>Paper Summary</title>
      <link>https://liyuanlucasliu.github.io/tags/paper-summary/</link>
    </image>
    
    <item>
      <title>One Sentence Summary for EMNLP 2020</title>
      <link>https://liyuanlucasliu.github.io/blog/2020-11-emnlp/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://liyuanlucasliu.github.io/blog/2020-11-emnlp/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding
&lt;ul&gt;
&lt;li&gt;Position encoding learns local position information in the encoder and absolute position in the decoder.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A Matter of Framing: The Impact of Linguistic Formalism on Probing Results
&lt;ul&gt;
&lt;li&gt;Choice of linguistic formalism is important for role-semantic probing results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contrastive Distillation on Intermediate Representations for Language Model Compression
&lt;ul&gt;
&lt;li&gt;Language model compression via contrastive loss, while memory bank is used to store negative samples (not updated in training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Efficient Meta Lifelong-Learning with Limited Memory
&lt;ul&gt;
&lt;li&gt;Efficient experience rehearsal is achieved by learn to select examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks
&lt;ul&gt;
&lt;li&gt;Introduced optimization-based meta-learning to language modeling pretraining-finetuning paradigm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference
&lt;ul&gt;
&lt;li&gt;Multi-head attention can be viewed as multiple sample from the same distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Token-level Adaptive Training for Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Using frequency-based heuristic to re-weight the loss value for different tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shallow-to-Deep Training for Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Both copying-strategy and lr-reset matters for progressive Transformer-based NMT training (copying is more important and &lt;code&gt;123-&amp;gt;123123&lt;/code&gt; &amp;gt; &lt;code&gt;123-&amp;gt;112233&lt;/code&gt; &amp;gt; &lt;code&gt;123-&amp;gt;123333&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Incorporating a Local Translation Mechanism into Non-autoregressive Translation
&lt;ul&gt;
&lt;li&gt;In NAT, treating an autoregressive sequence instead of a token as the basic unit (for &lt;code&gt;1234&lt;/code&gt; generate &lt;code&gt;12&lt;/code&gt; and &lt;code&gt;34&lt;/code&gt; simultaneously while &lt;code&gt;2&lt;/code&gt; is generated after &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;To alleviate error accumulation, use attention mask to change classical attention (global, all tokens are accessible) to local attention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masking as an Efficient Alternative to Finetuning for Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;Instead of finetuning LMs, learn a weight mask instead (e.g., &lt;code&gt;[0.1, 0.3] -&amp;gt; [0.0, 0.3]&lt;/code&gt;), which is argued to be more memory-efficient (1-bit instead of 32/16-bit additional copy).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If Beam Search is the Answer, What was the Question?
&lt;ul&gt;
&lt;li&gt;Beam search has an inductive bias which can be linked to the promotion of uniform information
density &amp;mdash; variance of surprisals is linear to BLEU (people dont want surprise in reading).&lt;/li&gt;
&lt;li&gt;By adding regularization on information density, exact search performs similar to beam search.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When BERT Plays the Lottery, All Tickets Are Winning
&lt;ul&gt;
&lt;li&gt;The pruned “good” subnetworks works well, while the “bad” ones do not.&lt;/li&gt;
&lt;li&gt;For structured pruning, even “bad” subnetworks can be finetuned separately to reach fairly strong performance.&lt;/li&gt;
&lt;li&gt;Different runs get different “good” subnetworks, indicating the existance of factors otherthan non-trivial linguistic features.&lt;/li&gt;
&lt;li&gt;The success of BERT might be more related to optimization surfaces rather than specific bits
of linguistic knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning
&lt;ul&gt;
&lt;li&gt;Using RL to select context for NMT at the document-level.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Pruning in training is much better than pruning after converge (0.1 BLEU drop v.s. 1.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT
&lt;ul&gt;
&lt;li&gt;Pre-training on high-resource -&amp;gt; fine-tune on low-resource by expanding dictionary -&amp;gt; XLM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems
&lt;ul&gt;
&lt;li&gt;Char-level NMT &amp;lt; first train Subword-level NMT, then convert to Character-level &amp;lt; Subword-level NMT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Jointly learn to select training data from comparable (rather than parallel) data, and to NMT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can Automatic Post-Editing Improve NMT?
&lt;ul&gt;
&lt;li&gt;With more training data, automatic post-editing can improve NMT a lot.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning from Context or Names? An Empirical Study on Neural Relation Extraction
&lt;ul&gt;
&lt;li&gt;Both context and names are important for performance, while existing RE datasets may leak cues via names.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Some Languages Seem Easier to Parse Because Their Treebanks Leak
&lt;ul&gt;
&lt;li&gt;Some languages seem easier to parse, since the trees in the test set are mostly isomorphic with some tree in the training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Word Frequency Does Not Predict Grammatical Knowledge in Language
Models
&lt;ul&gt;
&lt;li&gt;Comparing &lt;code&gt;The cat walks&lt;/code&gt; and &lt;code&gt;The cat walk&lt;/code&gt;, the author found in four dimensions, frequency is not related to how well the knowledge is learned.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do sequence-to-sequence VAEs learn global features of sentences?
&lt;ul&gt;
&lt;li&gt;VAEs are prone to memorizing the first words and the sentence length, producing local
features of limited usefulness, while chaning architecture can help to alleviate such memorization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial Semantic Collisions
&lt;ul&gt;
&lt;li&gt;Semantic collisions refer to texts that are semantically unrelated but judged as similar by NLP models, which can be effectively derived with gradient based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sparse Text Generation
&lt;ul&gt;
&lt;li&gt;Entmax transformation is leveraged to train and sample from a natively sparse language model, while \alpha-Entmax unifies softmax, argmax, and sparsemax.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning VariationalWord Masks to Improve the Interpretability of Neural Text Classifiers
&lt;ul&gt;
&lt;li&gt;word masks are automatically learned to emphasize task-specific words, which improves interpretability and performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Identifying Elements Essential for BERT’s Multilinguality
&lt;ul&gt;
&lt;li&gt;shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality.&lt;/li&gt;
&lt;li&gt;Word order is relevant: BERT is not multilingual with one language having an inverted word order.&lt;/li&gt;
&lt;li&gt;The comparability of training corpora contributes to multilinguality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A Streaming Approach For Efficient Batched Beam Search
&lt;ul&gt;
&lt;li&gt;Periodically “refills” the batch before proceeding with a selected subset of candidates, special handlings are made to handle the self-attention padding.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;On Losses for Modern Language Models
&lt;ul&gt;
&lt;li&gt;Clarify NSP’s effect on BERT pre-training and design more auxiliary tasks, which allows the final method outperforms BERTBase with fewer than a quarter of the training tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entities as Experts: Sparse Memory Access with Entity Supervision
&lt;ul&gt;
&lt;li&gt;Leveraging entity representations to better memorize sparse knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic Label Smoothing for Sequence to Sequence Problems
&lt;ul&gt;
&lt;li&gt;Retrieve semantically similar sentences to do label smoothing, at the cost of training computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We Can Detect Your Bias: Predicting the Political Ideology of News Articles
&lt;ul&gt;
&lt;li&gt;Directly using BERT would result in a heavy dependency on source, which motivates the leverage of adversarial training to neutralize such leaked clues.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Imitation Attacks and Defenses for Black-box Machine Translation Systems
&lt;ul&gt;
&lt;li&gt;After imitating black-box machine translation system, attack the imitated system with universal trigger attack / suffix dropper attack / targeted flip attack.&lt;/li&gt;
&lt;li&gt;Discussions are included to defense model stealing with gradient poisoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three
&lt;ul&gt;
&lt;li&gt;Ensemble helps to calibration and improve model performance, while distillation helps to make it faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Inference Strategies for Machine Translation with Conditional Masking
&lt;ul&gt;
&lt;li&gt;Using masked language model as decoder, and iterative replacing masks with model outputs.&lt;/li&gt;
&lt;li&gt;Four strategies are used: fixed #iterations; fixed #tokens/iteration; probs &amp;gt; T; comined-probs &amp;gt; T (best)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An Empirical Study of Generation Order for Machine Translation
&lt;ul&gt;
&lt;li&gt;Using Transformer to do generation in an insertive manner, i.e., &lt;code&gt;(a, c)-&amp;gt;b&lt;/code&gt;, many efforts are made on exploring different generation orders (balanced binary tree works the best, while others are not far behind)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems
&lt;ul&gt;
&lt;li&gt;Benchmark for hyper-parameter optimization on NMT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation
&lt;ul&gt;
&lt;li&gt;Learn to generate masking, for adapting language model to a new domain in a faster manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting
&lt;ul&gt;
&lt;li&gt;For small dataset, adding additional regularization on the parameter wight shift (difference to the original pre-trained weights)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning
&lt;ul&gt;
&lt;li&gt;After pre-training NER models on a source domain, trying to adapt the model to a new domain by conducting structured nearest neighbor search (similarities are calculated based on representations constructed by the pre-trained model).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks
&lt;ul&gt;
&lt;li&gt;After linearization labeled sentence (converting into &lt;code&gt;B-PER Jose E-PER&lt;/code&gt;), language modeling is trained and used to generate new data for the training.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models
&lt;ul&gt;
&lt;li&gt;Pre-training LSTMs on music helps to train it on language later, but not random generated texts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
&lt;ul&gt;
&lt;li&gt;LM does not perform well on numerical commonsense knowledge, even with distant supervision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Train No Evil: Selective Masking for Task-Guided Pre-Training
&lt;ul&gt;
&lt;li&gt;Finding important tokens with heuristic strategies and conducting task-guided pre-training (heuristic leverages task-specific information).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fact or Fiction: Verifying Scientific Claims
&lt;ul&gt;
&lt;li&gt;Propose a new task called scientific claim verification and construct corresponding datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language Model Prior for Low-Resource Neural Machine Translation
&lt;ul&gt;
&lt;li&gt;Using language model outputs to regularize NMT in a knowledge distillation manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
